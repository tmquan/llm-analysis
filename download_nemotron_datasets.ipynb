{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "20dce34f",
   "metadata": {},
   "source": [
    "# NVIDIA Nemotron Post-Training Datasets\n",
    "\n",
    "This notebook downloads and explores NVIDIA's Nemotron post-training datasets, which are used for fine-tuning large language models.\n",
    "\n",
    "## üìö Available Datasets\n",
    "\n",
    "1. **Nemotron-Post-Training-Dataset-v1**: Original version with chat, code, math, stem, and tool_calling splits\n",
    "2. **Nemotron-Post-Training-Dataset-v2**: Updated version with improvements\n",
    "3. **Llama-Nemotron-Post-Training-Dataset**: Comprehensive dataset with SFT and RL subsets\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf512a4",
   "metadata": {},
   "source": [
    "## üîß Setup\n",
    "\n",
    "First, install required packages and set up the environment.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a191028",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install datasets huggingface_hub ipywidgets -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57b9c6de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "import os\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "# Create datasets folder if it doesn't exist\n",
    "os.makedirs(\"datasets\", exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ba2549",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üì• Dataset Downloads\n",
    "\n",
    "### 1Ô∏è‚É£ Nemotron Post-Training Dataset v1\n",
    "\n",
    "Download the original Nemotron dataset with multiple domain-specific splits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feab2fb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîΩ Downloading Nemotron-Post-Training-Dataset-v1...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2d3a92b502f46ceb463faa3a43216ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f33b5b427adc4e54ba932a3b7f47f203",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "911c9f2b4051400897faf7d100d75e22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045a5c4647154ba69804e46274dd9cfd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc37551f166b4d0d8d337795eb40c7c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/159 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce145013fe464f9d9494ea41973759e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/660 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52886a6922144128978c9c2a6aaf2603",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/175 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7b036245b2624d4d8f6a9426cc32e071",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/152 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee22fe292f0a4c668163a5deea1fdeac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/649 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Download of v1 completed!\n",
      "   Dataset splits: ['chat', 'code', 'math', 'stem', 'tool_calling']\n",
      "   Total samples: 25,659,642\n",
      "   Location: /localhome/local-tranminhq/llm-analysis/datasets/nemotron-v1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v1\n",
    "print(\"üîΩ Downloading Nemotron-Post-Training-Dataset-v1...\\n\")\n",
    "\n",
    "dataset_v1 = load_dataset(\n",
    "    \"nvidia/Nemotron-Post-Training-Dataset-v1\",\n",
    "    cache_dir=\"./datasets/nemotron-v1\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Download of v1 completed!\")\n",
    "print(f\"   Dataset splits: {list(dataset_v1.keys())}\")\n",
    "print(f\"   Total samples: {sum(len(dataset_v1[split]) for split in dataset_v1.keys()):,}\")\n",
    "print(f\"   Location: {os.path.abspath('datasets/nemotron-v1')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe611fd",
   "metadata": {},
   "source": [
    "### 2Ô∏è‚É£ Nemotron Post-Training Dataset v2\n",
    "\n",
    "Download the updated version with improvements and refinements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ea6c888",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîΩ Downloading Nemotron-Post-Training-Dataset-v2...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce50a7cd8ae34dc59bfaf8d7b7f5be4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46255e467f2744eb9fe5006a6f3ca757",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ed1bbf08ae2e485eadb9588eb37cf312",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e8f852e297048ffa7ddd67bd0251de8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f3709cbd1c441308d6e1aa429bbfcaa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aff0a01a479d410db75c23a30335136e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2308d13a83d4e4790d77138502164b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "802c8d0d614545f48f3d863d9af1ffdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac3d5e5a915446dc876d119a1872ef18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5d15d185c9d540b4ba9a5b116c298d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b0e2dd601ef94f1bb754a8b284b48588",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "964e6dad5ad04c668749b29df80e7249",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5d80e63fe194fbcb3f5ca9bfbd45c97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/37 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "852b4cdfa5174db5a08ed67a878c2f0f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eae91d066d5f4654a1217632a2a80932",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Download of v2 completed!\n",
      "   Dataset splits: ['stem', 'chat', 'math', 'code', 'multilingual_ja', 'multilingual_de', 'multilingual_it', 'multilingual_es', 'multilingual_fr']\n",
      "   Total samples: 6,341,414\n",
      "   Location: /localhome/local-tranminhq/llm-analysis/datasets/nemotron-v2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/nvidia/Nemotron-Post-Training-Dataset-v2\n",
    "print(\"üîΩ Downloading Nemotron-Post-Training-Dataset-v2...\\n\")\n",
    "\n",
    "dataset_v2 = load_dataset(\n",
    "    \"nvidia/Nemotron-Post-Training-Dataset-v2\",\n",
    "    cache_dir=\"./datasets/nemotron-v2\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Download of v2 completed!\")\n",
    "print(f\"   Dataset splits: {list(dataset_v2.keys())}\")\n",
    "print(f\"   Total samples: {sum(len(dataset_v2[split]) for split in dataset_v2.keys()):,}\")\n",
    "print(f\"   Location: {os.path.abspath('datasets/nemotron-v2')}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f450c66",
   "metadata": {},
   "source": [
    "### 3Ô∏è‚É£ Llama-Nemotron Post-Training Dataset\n",
    "\n",
    "Download the comprehensive Llama-Nemotron dataset including both **SFT** (Supervised Fine-Tuning) and **RL** (Reinforcement Learning) subsets.\n",
    "\n",
    "**Note:** This is a large dataset and may take significant time and disk space.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb8c415e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîΩ Starting download of Llama-Nemotron-Post-Training-Dataset (SFT subset)...\n",
      "   Splits: math, code, science, chat, safety\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2507844c17674324bf7a3c25dc1486f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/91 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "558e16f39c7b4a15927f856982da48b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading dataset shards:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Download of SFT subset completed!\n",
      "   Dataset splits: ['code', 'math', 'science', 'chat', 'safety']\n",
      "   Total samples: 32,955,418\n",
      "\n",
      "üîΩ Starting download of Llama-Nemotron-Post-Training-Dataset (RL subset)...\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a759875d45a4087b6c89eaeee9c6f9c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating instruction_following split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "DatasetGenerationError",
     "evalue": "An error occurred while generating the dataset",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/builder.py:1834\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1833\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1834\u001b[39m     \u001b[43mwriter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwrite_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1835\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m CastError \u001b[38;5;28;01mas\u001b[39;00m cast_error:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/arrow_writer.py:714\u001b[39m, in \u001b[36mArrowWriter.write_table\u001b[39m\u001b[34m(self, pa_table, writer_batch_size)\u001b[39m\n\u001b[32m    713\u001b[39m pa_table = pa_table.combine_chunks()\n\u001b[32m--> \u001b[39m\u001b[32m714\u001b[39m pa_table = \u001b[43mtable_cast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpa_table\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_schema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    715\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.embed_local_files:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:2272\u001b[39m, in \u001b[36mtable_cast\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2271\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m table.schema != schema:\n\u001b[32m-> \u001b[39m\u001b[32m2272\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcast_table_to_schema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mschema\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2273\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m table.schema.metadata != schema.metadata:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:2224\u001b[39m, in \u001b[36mcast_table_to_schema\u001b[39m\u001b[34m(table, schema)\u001b[39m\n\u001b[32m   2218\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m CastError(\n\u001b[32m   2219\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(table.schema)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(features)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mbecause column names don\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt match\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   2220\u001b[39m         table_column_names=table.column_names,\n\u001b[32m   2221\u001b[39m         requested_column_names=\u001b[38;5;28mlist\u001b[39m(features),\n\u001b[32m   2222\u001b[39m     )\n\u001b[32m   2223\u001b[39m arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2224\u001b[39m     \u001b[43mcast_array_to_feature\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2225\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtable_column_names\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mpa\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mtype\u001b[39;49m\u001b[43m=\u001b[49m\u001b[43mschema\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtype\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2226\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2227\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2228\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, feature \u001b[38;5;129;01min\u001b[39;00m features.items()\n\u001b[32m   2229\u001b[39m ]\n\u001b[32m   2230\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_arrays(arrays, schema=schema)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:1795\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1794\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(array, pa.ChunkedArray):\n\u001b[32m-> \u001b[39m\u001b[32m1795\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pa.chunked_array([\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m array.chunks])\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:2002\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2000\u001b[39m null_array = pa.array([\u001b[38;5;28;01mNone\u001b[39;00m] * \u001b[38;5;28mlen\u001b[39m(array))\n\u001b[32m   2001\u001b[39m arrays = [\n\u001b[32m-> \u001b[39m\u001b[32m2002\u001b[39m     \u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfield\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43marray_fields\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mnull_array\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msubfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2003\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m name, subfeature \u001b[38;5;129;01min\u001b[39;00m feature.items()\n\u001b[32m   2004\u001b[39m ]\n\u001b[32m   2005\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.StructArray.from_arrays(arrays, names=\u001b[38;5;28mlist\u001b[39m(feature), mask=array.is_null())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:1797\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:2052\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2051\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2052\u001b[39m     casted_array_values = \u001b[43m_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfeature\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2053\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m pa.types.is_list(array.type) \u001b[38;5;129;01mand\u001b[39;00m casted_array_values.type == array.values.type:\n\u001b[32m   2054\u001b[39m         \u001b[38;5;66;03m# Both array and feature have equal list type and values (within the list) type\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:1797\u001b[39m, in \u001b[36m_wrap_for_chunked_arrays.<locals>.wrapper\u001b[39m\u001b[34m(array, *args, **kwargs)\u001b[39m\n\u001b[32m   1796\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1797\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:2092\u001b[39m, in \u001b[36mcast_array_to_feature\u001b[39m\u001b[34m(array, feature, allow_primitive_to_str, allow_decimal_to_str)\u001b[39m\n\u001b[32m   2086\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m array_cast(\n\u001b[32m   2087\u001b[39m         array,\n\u001b[32m   2088\u001b[39m         feature(),\n\u001b[32m   2089\u001b[39m         allow_primitive_to_str=allow_primitive_to_str,\n\u001b[32m   2090\u001b[39m         allow_decimal_to_str=allow_decimal_to_str,\n\u001b[32m   2091\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m2092\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mCouldn\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt cast array of type\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(array.type)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mto\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00m_short_str(feature)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mTypeError\u001b[39m: Couldn't cast array of type\nstruct<num_bullets: int64, num_highlights: int64, postscript_marker: string, keywords: list<item: string>, num_sentences: int64, relation: string, num_words: int64, section_spliter: string, num_sections: int64, num_placeholders: int64, num_paragraphs: int64, end_phrase: string, keyword: string, frequency: int64, forbidden_words: list<item: string>, letter: string, let_frequency: int64, let_relation: string, capital_frequency: int64, capital_relation: string>\nto\n{'num_sentences': Value('int64'), 'relation': Value('string'), 'postscript_marker': Value('string'), 'num_bullets': Value('int64'), 'keywords': List(Value('string')), 'num_words': Value('int64'), 'forbidden_words': List(Value('string')), 'num_highlights': Value('int64'), 'section_spliter': Value('string'), 'num_sections': Value('int64'), 'num_paragraphs': Value('int64'), 'num_placeholders': Value('int64'), 'keyword': Value('string'), 'frequency': Value('int64'), 'end_phrase': Value('string'), 'letter': Value('string'), 'let_frequency': Value('int64'), 'let_relation': Value('string'), 'nth_paragraph': Value('int64'), 'first_word': Value('string')}",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mDatasetGenerationError\u001b[39m                    Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 20\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Download RL subset\u001b[39;00m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33müîΩ Starting download of Llama-Nemotron-Post-Training-Dataset (RL subset)...\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m dataset_rl = \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mnvidia/Llama-Nemotron-Post-Training-Dataset\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     22\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRL\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     23\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_dir\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m./datasets/llama-nemotron\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m     24\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m‚úÖ Download of RL subset completed!\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m   Dataset splits: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(dataset_rl.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/load.py:1417\u001b[39m, in \u001b[36mload_dataset\u001b[39m\u001b[34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[39m\n\u001b[32m   1414\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance.as_streaming_dataset(split=split)\n\u001b[32m   1416\u001b[39m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1417\u001b[39m \u001b[43mbuilder_instance\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1418\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1419\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1420\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1421\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1422\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1423\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1425\u001b[39m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[32m   1426\u001b[39m keep_in_memory = (\n\u001b[32m   1427\u001b[39m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance.info.dataset_size)\n\u001b[32m   1428\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/builder.py:897\u001b[39m, in \u001b[36mDatasetBuilder.download_and_prepare\u001b[39m\u001b[34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[39m\n\u001b[32m    895\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m num_proc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    896\u001b[39m     prepare_split_kwargs[\u001b[33m\"\u001b[39m\u001b[33mnum_proc\u001b[39m\u001b[33m\"\u001b[39m] = num_proc\n\u001b[32m--> \u001b[39m\u001b[32m897\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_download_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    898\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    899\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    900\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    901\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mdownload_and_prepare_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    902\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    903\u001b[39m \u001b[38;5;66;03m# Sync info\u001b[39;00m\n\u001b[32m    904\u001b[39m \u001b[38;5;28mself\u001b[39m.info.dataset_size = \u001b[38;5;28msum\u001b[39m(split.num_bytes \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.info.splits.values())\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/builder.py:973\u001b[39m, in \u001b[36mDatasetBuilder._download_and_prepare\u001b[39m\u001b[34m(self, dl_manager, verification_mode, **prepare_split_kwargs)\u001b[39m\n\u001b[32m    969\u001b[39m split_dict.add(split_generator.split_info)\n\u001b[32m    971\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    972\u001b[39m     \u001b[38;5;66;03m# Prepare split will record examples associated to the split\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m973\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_generator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mprepare_split_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    974\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    975\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[32m    976\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mCannot find data file. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    977\u001b[39m         + (\u001b[38;5;28mself\u001b[39m.manual_download_instructions \u001b[38;5;129;01mor\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    978\u001b[39m         + \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mOriginal error:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    979\u001b[39m         + \u001b[38;5;28mstr\u001b[39m(e)\n\u001b[32m    980\u001b[39m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/builder.py:1705\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split\u001b[39m\u001b[34m(self, split_generator, file_format, num_proc, max_shard_size)\u001b[39m\n\u001b[32m   1703\u001b[39m job_id = \u001b[32m0\u001b[39m\n\u001b[32m   1704\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m pbar:\n\u001b[32m-> \u001b[39m\u001b[32m1705\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_split_single\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1706\u001b[39m \u001b[43m        \u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgen_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mjob_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43m_prepare_split_args\u001b[49m\n\u001b[32m   1707\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1708\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdone\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m   1709\u001b[39m \u001b[43m            \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontent\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/builder.py:1861\u001b[39m, in \u001b[36mArrowBasedBuilder._prepare_split_single\u001b[39m\u001b[34m(self, gen_kwargs, fpath, file_format, max_shard_size, job_id)\u001b[39m\n\u001b[32m   1859\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(e, DatasetGenerationError):\n\u001b[32m   1860\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1861\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m DatasetGenerationError(\u001b[33m\"\u001b[39m\u001b[33mAn error occurred while generating the dataset\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m   1863\u001b[39m \u001b[38;5;28;01myield\u001b[39;00m job_id, \u001b[38;5;28;01mTrue\u001b[39;00m, (total_num_examples, total_num_bytes, writer._features, num_shards, shard_lengths)\n",
      "\u001b[31mDatasetGenerationError\u001b[39m: An error occurred while generating the dataset"
     ]
    }
   ],
   "source": [
    "# https://huggingface.co/datasets/nvidia/Llama-Nemotron-Post-Training-Dataset\n",
    "\n",
    "# Download SFT subset\n",
    "print(\"üîΩ Starting download of Llama-Nemotron-Post-Training-Dataset (SFT subset)...\")\n",
    "print(\"   Splits: math, code, science, chat, safety\\n\")\n",
    "\n",
    "dataset_sft = load_dataset(\n",
    "    \"nvidia/Llama-Nemotron-Post-Training-Dataset\",\n",
    "    \"SFT\",\n",
    "    cache_dir=\"./datasets/llama-nemotron\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Download of SFT subset completed!\")\n",
    "print(f\"   Dataset splits: {list(dataset_sft.keys())}\")\n",
    "print(f\"   Total samples: {sum(len(dataset_sft[split]) for split in dataset_sft.keys()):,}\\n\")\n",
    "\n",
    "# Download RL subset\n",
    "print(\"üîΩ Starting download of Llama-Nemotron-Post-Training-Dataset (RL subset)...\\n\")\n",
    "\n",
    "dataset_rl = load_dataset(\n",
    "    \"nvidia/Llama-Nemotron-Post-Training-Dataset\",\n",
    "    \"RL\",\n",
    "    cache_dir=\"./datasets/llama-nemotron\"\n",
    ")\n",
    "\n",
    "print(f\"\\n‚úÖ Download of RL subset completed!\")\n",
    "print(f\"   Dataset splits: {list(dataset_rl.keys())}\")\n",
    "print(f\"   Total samples: {sum(len(dataset_rl[split]) for split in dataset_rl.keys()):,}\\n\")\n",
    "\n",
    "display(Markdown(\"### ‚úÖ Llama-Nemotron dataset downloaded successfully!\"))\n",
    "display(Markdown(f\"**Location:** `{os.path.abspath('datasets/llama-nemotron')}`\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "024465f6",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìä Dataset Exploration\n",
    "\n",
    "Explore the structure and content of the downloaded datasets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bdd1e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display summary of all datasets\n",
    "print(\"=\" * 80)\n",
    "print(\"üìã DATASET SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "datasets_info = [\n",
    "    (\"Nemotron v1\", dataset_v1),\n",
    "    (\"Nemotron v2\", dataset_v2),\n",
    "    (\"Llama-Nemotron SFT\", dataset_sft),\n",
    "    (\"Llama-Nemotron RL\", dataset_rl)\n",
    "]\n",
    "\n",
    "for name, dataset in datasets_info:\n",
    "    if dataset:\n",
    "        print(f\"\\n{name}:\")\n",
    "        print(f\"  Splits: {', '.join(dataset.keys())}\")\n",
    "        print(f\"  Total samples: {sum(len(dataset[split]) for split in dataset.keys()):,}\")\n",
    "        for split in dataset.keys():\n",
    "            print(f\"    - {split}: {len(dataset[split]):,} samples\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6192bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine a sample from the SFT dataset\n",
    "print(\"üîç Sample from Llama-Nemotron SFT (Math split):\\n\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "if dataset_sft and 'math' in dataset_sft:\n",
    "    sample = dataset_sft['math'][0]\n",
    "    for key, value in sample.items():\n",
    "        print(f\"\\n{key}:\")\n",
    "        print(\"-\" * 40)\n",
    "        if isinstance(value, str) and len(value) > 500:\n",
    "            print(value[:500] + \"...\")\n",
    "        else:\n",
    "            print(value)\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b09123",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Additional exploration and analysis can be done here\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
