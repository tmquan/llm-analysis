{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# GPU-Accelerated Embedding Visualization with RAPIDS\n",
        "\n",
        "This notebook loads pre-extracted embeddings and visualizes them using **RAPIDS cuML** for GPU-accelerated UMAP/t-SNE dimensionality reduction and **Plotly** for interactive 2D/3D visualizations.\n",
        "\n",
        "## üìä Overview\n",
        "- **Data Source**: Pre-extracted embeddings from `embeddings_output/` (parquet files)\n",
        "- **GPU Acceleration**: RAPIDS cuDF + cuML for 100x faster processing\n",
        "- **Algorithms**: cuML UMAP and t-SNE (GPU-accelerated)\n",
        "- **Visualization**: Interactive 2D/3D Plotly scatter plots\n",
        "- **Labels**: Flexible labeling from dataset metadata, cuBERT, or None\n",
        "\n",
        "## üéØ Features\n",
        "- ‚úÖ **GPU-Accelerated**: cuML UMAP/t-SNE runs entirely on GPU\n",
        "- ‚úÖ **Memory Efficient**: cuDF for GPU DataFrame operations\n",
        "- ‚úÖ **Scalable**: Handle millions of embeddings efficiently\n",
        "- ‚úÖ **Interactive**: Plotly 2D/3D visualizations with hover details\n",
        "- ‚úÖ **Flexible Labels**: Support for metadata labels, cuBERT clustering, or unlabeled\n",
        "- ‚úÖ **Export**: Save to HTML for easy sharing\n",
        "\n",
        "## üîó Reference\n",
        "Based on [RAPIDS cuBERT Topic Modelling](https://github.com/rapidsai/rapids-examples/tree/main/cuBERT_topic_modelling)\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup: Install RAPIDS and Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n",
            "‚úÖ RAPIDS (cuDF, cuML) is available - GPU acceleration enabled!\n"
          ]
        }
      ],
      "source": [
        "# Install RAPIDS and required packages\n",
        "# Note: RAPIDS requires specific CUDA versions. See: https://rapids.ai/start.html\n",
        "# For CUDA 12.x:\n",
        "# !pip install cudf-cu13 cuml-cu13 --extra-index-url=https://pypi.nvidia.com\n",
        "\n",
        "# Core dependencies (non-RAPIDS fallback available)\n",
        "%pip install numpy pandas plotly tqdm pyarrow -q\n",
        "\n",
        "# Check if RAPIDS is available\n",
        "try:\n",
        "    import cudf\n",
        "    import cuml\n",
        "    RAPIDS_AVAILABLE = True\n",
        "    print(\"‚úÖ RAPIDS (cuDF, cuML) is available - GPU acceleration enabled!\")\n",
        "except ImportError:\n",
        "    RAPIDS_AVAILABLE = False\n",
        "    print(\"‚ö†Ô∏è  RAPIDS not available - falling back to CPU (numpy/sklearn)\")\n",
        "    print(\"   To install RAPIDS: pip install cudf-cu13 cuml-cu13 --extra-index-url=https://pypi.nvidia.com\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ RAPIDS imports successful (cuDF, cuML UMAP/TSNE)\n",
            "\n",
            "üñ•Ô∏è  Using: GPU (RAPIDS cuML)\n",
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from pathlib import Path\n",
        "from tqdm.auto import tqdm\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# RAPIDS imports (with CPU fallback)\n",
        "if RAPIDS_AVAILABLE:\n",
        "    import cudf\n",
        "    import cupy as cp\n",
        "    from cuml.manifold import UMAP as cumlUMAP\n",
        "    from cuml.manifold import TSNE as cumlTSNE\n",
        "    print(\"‚úÖ RAPIDS imports successful (cuDF, cuML UMAP/TSNE)\")\n",
        "else:\n",
        "    # CPU fallback\n",
        "    try:\n",
        "        from sklearn.manifold import TSNE as sklearnTSNE\n",
        "        import umap as cpuUMAP\n",
        "        print(\"‚úÖ CPU fallback imports successful (sklearn TSNE, umap-learn)\")\n",
        "    except ImportError:\n",
        "        print(\"‚ö†Ô∏è  Installing CPU fallback packages...\")\n",
        "        import subprocess\n",
        "        subprocess.run([\"pip\", \"install\", \"umap-learn\", \"scikit-learn\", \"-q\"])\n",
        "        from sklearn.manifold import TSNE as sklearnTSNE\n",
        "        import umap as cpuUMAP\n",
        "\n",
        "# Configure Plotly for notebook rendering\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook_connected\"\n",
        "\n",
        "print(f\"\\nüñ•Ô∏è  Using: {'GPU (RAPIDS cuML)' if RAPIDS_AVAILABLE else 'CPU (sklearn/umap-learn)'}\")\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ‚öôÔ∏è Configuration\n",
        "\n",
        "Configure paths, dataset selection, and visualization parameters.\n",
        "\n",
        "### üìÇ Path Options\n",
        "\n",
        "| Option | When to Use | How to Configure |\n",
        "|--------|-------------|------------------|\n",
        "| **1. Direct Paths** | Simple, one-time setup | Edit paths directly in the cell below |\n",
        "| **2. Relative Paths** | Portable, stays with notebook | Uncomment the relative path block |\n",
        "| **3. Environment Variables** | CI/CD, shared environments | Set `EMBEDDINGS_DIR`, `DATASETS_DIR`, `OUTPUT_DIR` in shell |\n",
        "\n",
        "### üìä Dataset/Split Selection (similar to `extract_embeddings_parallel_shards.py`)\n",
        "\n",
        "| Option | Example | Description |\n",
        "|--------|---------|-------------|\n",
        "| **A. Load All** | `LOAD_ALL = True` | Load all available embeddings |\n",
        "| **B. Specific** | `[\"v1:chat\", \"v2:math\"]` | Select exact dataset:split combinations |\n",
        "| **C. By Dataset** | `[\"v1:*\", \"v2:*\"]` | All splits from specified datasets |\n",
        "| **D. By Split** | `[\"*:chat\", \"*:code\"]` | Same split across all datasets |\n",
        "| **E. Mixed** | `[\"v1:*\", \"*:safety\"]` | Combine patterns |\n",
        "\n",
        "### Quick Edit:\n",
        "```python\n",
        "# Paths\n",
        "EMBEDDINGS_DIR = Path(\"/your/path/to/embeddings\")\n",
        "DATASETS_DIR = Path(\"/your/path/to/datasets\")  \n",
        "OUTPUT_DIR = Path(\"/your/path/to/outputs\")\n",
        "\n",
        "# Selection\n",
        "LOAD_ALL = False\n",
        "SELECTED_SPLITS = [\"v1:chat\", \"v1:code\", \"llama-sft:*\"]\n",
        "```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìÇ Path Configuration:\n",
            "   EMBEDDINGS_DIR: /raid/embeddings\n",
            "   DATASETS_DIR:   /raid/datasets\n",
            "   OUTPUT_DIR:     /raid/outputs\n",
            "   ‚úÖ OUTPUT_DIR created/verified\n",
            "\n",
            "üìä Data Selection:\n",
            "   Mode: Selected splits only\n",
            "      ‚Ä¢ v2:chat\n",
            "      ‚Ä¢ v2:code\n",
            "      ‚Ä¢ v2:math\n",
            "      ‚Ä¢ v2:stem\n",
            "\n",
            "‚öôÔ∏è  Visualization Settings:\n",
            "   Reduction method: umap\n",
            "   Sample size: All data\n",
            "   UMAP: n_neighbors=15, min_dist=0.1, metric=cosine\n",
            "   t-SNE: perplexity=30, learning_rate=200\n"
          ]
        }
      ],
      "source": [
        "# =============================================================================\n",
        "# CONFIGURATION - EDIT PATHS HERE\n",
        "# =============================================================================\n",
        "\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ OPTION 1: Direct Paths (Recommended)                                        ‚îÇ\n",
        "# ‚îÇ Simply set absolute paths to your data directories                          ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "\n",
        "EMBEDDINGS_DIR = Path(\"/raid/embeddings\")\n",
        "DATASETS_DIR = Path(\"/raid/datasets\")\n",
        "OUTPUT_DIR = Path(\"/raid/outputs\")\n",
        "\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ OPTION 2: Relative Paths (uncomment to use)                                 ‚îÇ\n",
        "# ‚îÇ Paths relative to notebook location                                         ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "# SCRIPT_DIR = Path(\".\").absolute()\n",
        "# EMBEDDINGS_DIR = SCRIPT_DIR / \"embeddings\"\n",
        "# DATASETS_DIR = SCRIPT_DIR / \"datasets\"\n",
        "# OUTPUT_DIR = SCRIPT_DIR / \"outputs\"\n",
        "\n",
        "# ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "# ‚îÇ OPTION 3: Environment Variables (uncomment to use)                          ‚îÇ\n",
        "# ‚îÇ Set via: export EMBEDDINGS_DIR=/path/to/embeddings                          ‚îÇ\n",
        "# ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "# EMBEDDINGS_DIR = Path(os.environ.get(\"EMBEDDINGS_DIR\", \"./embeddings\"))\n",
        "# DATASETS_DIR = Path(os.environ.get(\"DATASETS_DIR\", \"./datasets\"))\n",
        "# OUTPUT_DIR = Path(os.environ.get(\"OUTPUT_DIR\", \"./outputs\"))\n",
        "\n",
        "# =============================================================================\n",
        "# DATASET / SPLIT SELECTION (similar to extract_embeddings_parallel_shards.py)\n",
        "# =============================================================================\n",
        "# Format: \"dataset:split\" - specify exactly which data to visualize\n",
        "#\n",
        "# OPTION A: Load ALL available embeddings\n",
        "# LOAD_ALL = True\n",
        "# SELECTED_SPLITS = []  # Ignored when LOAD_ALL = True\n",
        "\n",
        "# OPTION B: Select specific dataset:split combinations (set LOAD_ALL = False)\n",
        "# LOAD_ALL = False\n",
        "# SELECTED_SPLITS = [\n",
        "#     \"v1:chat\",\n",
        "#     \"v1:code\", \n",
        "#     \"v1:math\",\n",
        "#     \"v2:stem\",\n",
        "#     \"llama-sft:safety\",\n",
        "#     \"llama-sft:science\",\n",
        "# ]\n",
        "\n",
        "# OPTION C: Select by dataset only (all splits from those datasets)\n",
        "# LOAD_ALL = False\n",
        "# SELECTED_SPLITS = [\"v1:*\", \"v2:*\"]  # All splits from v1 and v2\n",
        "\n",
        "# OPTION D: Select by split only (same split across all datasets)\n",
        "# LOAD_ALL = False  \n",
        "# SELECTED_SPLITS = [\"*:chat\", \"*:code\"]  # All chat and code splits\n",
        "\n",
        "# OPTION E: Mix and match\n",
        "# LOAD_ALL = False\n",
        "# SELECTED_SPLITS = [\n",
        "#     \"v1:*\",           # All v1 splits\n",
        "#     \"v2:chat\",        # Only v2 chat\n",
        "#     \"*:safety\",       # Safety from all datasets\n",
        "#     \"llama-sft:code\", # Specific combination\n",
        "# ]\n",
        "\n",
        "LOAD_ALL = False\n",
        "SELECTED_SPLITS = [\n",
        "    # \"llama-sft:chat\", \n",
        "    # \"llama-sft:code\",\n",
        "    # \"llama-sft:math\",\n",
        "    # \"llama-sft:science\",\n",
        "    # \"llama-sft:safety\",\n",
        "    # \"llama-sft:stem\",\n",
        "    # \"llama-sft:tool_calling\",\n",
        "    \"v2:chat\",\n",
        "    \"v2:code\",\n",
        "    \"v2:math\",\n",
        "    \"v2:stem\",\n",
        "    \n",
        "]  # Ignored when LOAD_ALL = True\n",
        "# =============================================================================\n",
        "# Validate paths exist\n",
        "# =============================================================================\n",
        "print(\"üìÇ Path Configuration:\")\n",
        "print(f\"   EMBEDDINGS_DIR: {EMBEDDINGS_DIR}\")\n",
        "print(f\"   DATASETS_DIR:   {DATASETS_DIR}\")\n",
        "print(f\"   OUTPUT_DIR:     {OUTPUT_DIR}\")\n",
        "\n",
        "# Check if paths exist\n",
        "if not EMBEDDINGS_DIR.exists():\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: EMBEDDINGS_DIR does not exist!\")\n",
        "if not DATASETS_DIR.exists():\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: DATASETS_DIR does not exist!\")\n",
        "\n",
        "# Create output directory\n",
        "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
        "print(f\"   ‚úÖ OUTPUT_DIR created/verified\")\n",
        "\n",
        "# Show selection mode\n",
        "print(f\"\\nüìä Data Selection:\")\n",
        "if LOAD_ALL:\n",
        "    print(f\"   Mode: Load ALL available embeddings\")\n",
        "else:\n",
        "    print(f\"   Mode: Selected splits only\")\n",
        "    for s in SELECTED_SPLITS:\n",
        "        print(f\"      ‚Ä¢ {s}\")\n",
        "\n",
        "# =============================================================================\n",
        "# VISUALIZATION SETTINGS\n",
        "# =============================================================================\n",
        "\n",
        "SAMPLE_SIZE = None  # None = use all data, or set a number like 100000 for testing\n",
        "RANDOM_SEED = 42\n",
        "\n",
        "# Dimensionality reduction settings\n",
        "REDUCTION_METHOD = \"umap\"  # \"umap\" or \"tsne\"\n",
        "N_COMPONENTS_2D = 2\n",
        "N_COMPONENTS_3D = 3\n",
        "\n",
        "# UMAP parameters (GPU-optimized)\n",
        "UMAP_N_NEIGHBORS = 15\n",
        "UMAP_MIN_DIST = 0.1\n",
        "UMAP_METRIC = \"cosine\"\n",
        "\n",
        "# t-SNE parameters (GPU-optimized)\n",
        "TSNE_PERPLEXITY = 30\n",
        "TSNE_LEARNING_RATE = 200\n",
        "\n",
        "# Color scheme for categories\n",
        "CATEGORY_COLORS = {\n",
        "    'chat': '#FF6B6B',\n",
        "    'code': '#4ECDC4', \n",
        "    'math': '#45B7D1',\n",
        "    'stem': '#96CEB4',\n",
        "    'tool_calling': '#FFEAA7',\n",
        "    'science': '#DDA0DD',\n",
        "    'safety': '#FF7F50',\n",
        "    # 'multilingual_ja': '#9B59B6',\n",
        "    # 'multilingual_de': '#3498DB',\n",
        "    # 'multilingual_it': '#E74C3C',\n",
        "    # 'multilingual_es': '#F39C12',\n",
        "    # 'multilingual_fr': '#1ABC9C',\n",
        "    'unknown': '#95A5A6'\n",
        "}\n",
        "\n",
        "print(\"\\n‚öôÔ∏è  Visualization Settings:\")\n",
        "print(f\"   Reduction method: {REDUCTION_METHOD}\")\n",
        "print(f\"   Sample size: {'All data' if SAMPLE_SIZE is None else f'{SAMPLE_SIZE:,}'}\")\n",
        "print(f\"   UMAP: n_neighbors={UMAP_N_NEIGHBORS}, min_dist={UMAP_MIN_DIST}, metric={UMAP_METRIC}\")\n",
        "print(f\"   t-SNE: perplexity={TSNE_PERPLEXITY}, learning_rate={TSNE_LEARNING_RATE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Load Extracted Embeddings\n",
        "\n",
        "Load pre-extracted embeddings from parquet files in `embeddings_output/` directory.\n",
        "These were generated by `extract_embeddings_parallel_shards.py`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def matches_selection(dataset: str, split: str, selected_splits: List[str]) -> bool:\n",
        "    \"\"\"\n",
        "    Check if a dataset:split combination matches any of the selection patterns.\n",
        "    \n",
        "    Supports wildcards:\n",
        "    - \"v1:chat\"     - exact match\n",
        "    - \"v1:*\"        - all splits from v1\n",
        "    - \"*:chat\"      - chat split from all datasets\n",
        "    - \"*:*\"         - everything (same as LOAD_ALL=True)\n",
        "    \n",
        "    Args:\n",
        "        dataset: Dataset name (e.g., \"v1\", \"llama-sft\")\n",
        "        split: Split name (e.g., \"chat\", \"code\")\n",
        "        selected_splits: List of selection patterns\n",
        "    \n",
        "    Returns:\n",
        "        True if matches any pattern\n",
        "    \"\"\"\n",
        "    for pattern in selected_splits:\n",
        "        if ':' not in pattern:\n",
        "            # Treat as dataset-only pattern\n",
        "            if pattern == dataset or pattern == '*':\n",
        "                return True\n",
        "            continue\n",
        "        \n",
        "        pat_dataset, pat_split = pattern.split(':', 1)\n",
        "        \n",
        "        # Check dataset match\n",
        "        dataset_match = (pat_dataset == '*' or pat_dataset == dataset)\n",
        "        \n",
        "        # Check split match\n",
        "        split_match = (pat_split == '*' or pat_split == split)\n",
        "        \n",
        "        if dataset_match and split_match:\n",
        "            return True\n",
        "    \n",
        "    return False\n",
        "\n",
        "\n",
        "def discover_embedding_files(\n",
        "    embeddings_dir: Path,\n",
        "    load_all: bool = True,\n",
        "    selected_splits: Optional[List[str]] = None\n",
        ") -> List[Dict]:\n",
        "    \"\"\"\n",
        "    Discover parquet embedding files with optional filtering.\n",
        "    \n",
        "    Args:\n",
        "        embeddings_dir: Path to embeddings directory\n",
        "        load_all: If True, load all available embeddings\n",
        "        selected_splits: List of \"dataset:split\" patterns to filter\n",
        "                        Supports wildcards: \"v1:*\", \"*:chat\", \"v1:chat\"\n",
        "    \n",
        "    Returns:\n",
        "        List of dicts with: dataset, split, shard_idx, total_shards, filepath\n",
        "    \"\"\"\n",
        "    files = []\n",
        "    \n",
        "    if not embeddings_dir.exists():\n",
        "        print(f\"‚ö†Ô∏è  Embeddings directory not found: {embeddings_dir}\")\n",
        "        print(\"   Run extract_embeddings_parallel_shards.py first to generate embeddings.\")\n",
        "        return files\n",
        "    \n",
        "    if selected_splits is None:\n",
        "        selected_splits = []\n",
        "    \n",
        "    # Track what we find vs what was requested\n",
        "    found_combinations = set()\n",
        "    \n",
        "    # Walk through embeddings directory\n",
        "    for dataset_dir in sorted(embeddings_dir.iterdir()):\n",
        "        if not dataset_dir.is_dir():\n",
        "            continue\n",
        "        \n",
        "        dataset_name = dataset_dir.name\n",
        "        \n",
        "        for split_dir in sorted(dataset_dir.iterdir()):\n",
        "            if not split_dir.is_dir():\n",
        "                continue\n",
        "            \n",
        "            split_name = split_dir.name\n",
        "            \n",
        "            # Check if this combination should be included\n",
        "            if not load_all and selected_splits:\n",
        "                if not matches_selection(dataset_name, split_name, selected_splits):\n",
        "                    continue\n",
        "            \n",
        "            found_combinations.add(f\"{dataset_name}:{split_name}\")\n",
        "            \n",
        "            # Find all parquet files\n",
        "            parquet_files = sorted(split_dir.glob(\"*.parquet\"))\n",
        "            \n",
        "            for pq_file in parquet_files:\n",
        "                # Parse filename: v1-chat-00000-of-00001.parquet\n",
        "                parts = pq_file.stem.split(\"-\")\n",
        "                if len(parts) >= 4 and \"of\" in parts:\n",
        "                    of_idx = parts.index(\"of\")\n",
        "                    shard_idx = int(parts[of_idx - 1])\n",
        "                    total_shards = int(parts[of_idx + 1])\n",
        "                else:\n",
        "                    shard_idx = 0\n",
        "                    total_shards = 1\n",
        "                \n",
        "                files.append({\n",
        "                    'dataset': dataset_name,\n",
        "                    'split': split_name,\n",
        "                    'shard_idx': shard_idx,\n",
        "                    'total_shards': total_shards,\n",
        "                    'filepath': pq_file\n",
        "                })\n",
        "    \n",
        "    # Show what was found\n",
        "    if not load_all and selected_splits:\n",
        "        print(f\"\\nüéØ Selection filter active:\")\n",
        "        for pattern in selected_splits:\n",
        "            print(f\"   ‚Ä¢ {pattern}\")\n",
        "        print(f\"\\n   Matched {len(found_combinations)} dataset:split combination(s)\")\n",
        "    \n",
        "    return files\n",
        "\n",
        "\n",
        "def load_embeddings_from_parquet(\n",
        "    file_infos: List[Dict],\n",
        "    sample_size: Optional[int] = None,\n",
        "    random_seed: int = 42\n",
        ") -> Tuple[np.ndarray, pd.DataFrame]:\n",
        "    \"\"\"\n",
        "    Load embeddings from parquet files into numpy array and metadata DataFrame.\n",
        "    \n",
        "    Args:\n",
        "        file_infos: List of file info dicts from discover_embedding_files()\n",
        "        sample_size: Optional limit on total samples to load\n",
        "        random_seed: Random seed for sampling\n",
        "    \n",
        "    Returns:\n",
        "        Tuple of (embeddings_array, metadata_df)\n",
        "    \"\"\"\n",
        "    all_embeddings = []\n",
        "    all_metadata = []\n",
        "    \n",
        "    np.random.seed(random_seed)\n",
        "    \n",
        "    print(f\"üìÇ Loading embeddings from {len(file_infos)} parquet file(s)...\")\n",
        "    \n",
        "    for file_info in tqdm(file_infos, desc=\"Loading files\"):\n",
        "        filepath = file_info['filepath']\n",
        "        \n",
        "        try:\n",
        "            # Load parquet file\n",
        "            if RAPIDS_AVAILABLE:\n",
        "                df = cudf.read_parquet(str(filepath))\n",
        "                # Convert embeddings column to numpy\n",
        "                embeddings = df['embeddings'].to_pandas().tolist()\n",
        "                indices = df['original_index'].to_pandas().tolist() if 'original_index' in df.columns else list(range(len(df)))\n",
        "            else:\n",
        "                df = pd.read_parquet(str(filepath))\n",
        "                embeddings = df['embeddings'].tolist()\n",
        "                indices = df['original_index'].tolist() if 'original_index' in df.columns else list(range(len(df)))\n",
        "            \n",
        "            # Add embeddings and metadata\n",
        "            for i, (emb, idx) in enumerate(zip(embeddings, indices)):\n",
        "                all_embeddings.append(emb)\n",
        "                all_metadata.append({\n",
        "                    'dataset': file_info['dataset'],\n",
        "                    'split': file_info['split'],\n",
        "                    'shard_idx': file_info['shard_idx'],\n",
        "                    'original_index': idx,\n",
        "                    'label': file_info['split']  # Default label = split name\n",
        "                })\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"   ‚ö†Ô∏è  Error loading {filepath.name}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    if not all_embeddings:\n",
        "        raise ValueError(\"No embeddings loaded! Check the embeddings directory.\")\n",
        "    \n",
        "    # Convert to numpy array\n",
        "    embeddings_array = np.array(all_embeddings, dtype=np.float32)\n",
        "    metadata_df = pd.DataFrame(all_metadata)\n",
        "    \n",
        "    # Sample if requested\n",
        "    if sample_size is not None and sample_size < len(embeddings_array):\n",
        "        print(f\"   üìä Sampling {sample_size:,} from {len(embeddings_array):,} embeddings...\")\n",
        "        indices = np.random.choice(len(embeddings_array), size=sample_size, replace=False)\n",
        "        embeddings_array = embeddings_array[indices]\n",
        "        metadata_df = metadata_df.iloc[indices].reset_index(drop=True)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Loaded {len(embeddings_array):,} embeddings\")\n",
        "    print(f\"   Embedding dimension: {embeddings_array.shape[1]}\")\n",
        "    print(f\"   Datasets: {metadata_df['dataset'].unique().tolist()}\")\n",
        "    print(f\"   Splits: {metadata_df['split'].unique().tolist()}\")\n",
        "    \n",
        "    return embeddings_array, metadata_df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üìÇ Discovering embedding files...\n",
            "================================================================================\n",
            "\n",
            "üéØ Selection filter active:\n",
            "   ‚Ä¢ v2:chat\n",
            "   ‚Ä¢ v2:code\n",
            "   ‚Ä¢ v2:math\n",
            "   ‚Ä¢ v2:stem\n",
            "\n",
            "   Matched 4 dataset:split combination(s)\n",
            "\n",
            "üìã Found 18 embedding file(s):\n",
            "\n",
            "   v2/chat: 12 shard(s) of 12\n",
            "   v2/code: 2 shard(s) of 2\n",
            "   v2/math: 2 shard(s) of 2\n",
            "   v2/stem: 2 shard(s) of 2\n",
            "\n",
            "================================================================================\n",
            "üìÇ Loading embeddings from 18 parquet file(s)...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7957d7ee1b7844acb92f3930a76585b5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading files:   0%|          | 0/18 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Loaded 1,397,187 embeddings\n",
            "   Embedding dimension: 4096\n",
            "   Datasets: ['v2']\n",
            "   Splits: ['chat', 'code', 'math', 'stem']\n",
            "================================================================================\n",
            "\n",
            "üìä Data Distribution:\n",
            "\n",
            "By Dataset:\n",
            "dataset\n",
            "v2    1397187\n",
            "\n",
            "By Split (Label):\n",
            "split\n",
            "chat    627720\n",
            "stem    355000\n",
            "math    239467\n",
            "code    175000\n"
          ]
        }
      ],
      "source": [
        "# Discover and load embeddings\n",
        "print(\"=\" * 80)\n",
        "print(\"üìÇ Discovering embedding files...\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use selection parameters from configuration\n",
        "file_infos = discover_embedding_files(\n",
        "    EMBEDDINGS_DIR,\n",
        "    load_all=LOAD_ALL,\n",
        "    selected_splits=SELECTED_SPLITS\n",
        ")\n",
        "\n",
        "if file_infos:\n",
        "    # Show discovered files\n",
        "    print(f\"\\nüìã Found {len(file_infos)} embedding file(s):\\n\")\n",
        "    \n",
        "    # Group by dataset and split\n",
        "    from collections import defaultdict\n",
        "    grouped = defaultdict(list)\n",
        "    for f in file_infos:\n",
        "        grouped[f\"{f['dataset']}/{f['split']}\"].append(f)\n",
        "    \n",
        "    for key, files in sorted(grouped.items()):\n",
        "        total_shards = files[0]['total_shards']\n",
        "        print(f\"   {key}: {len(files)} shard(s) of {total_shards}\")\n",
        "    \n",
        "    # Load embeddings\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    embeddings, metadata_df = load_embeddings_from_parquet(file_infos, sample_size=SAMPLE_SIZE)\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # Display metadata distribution\n",
        "    print(\"\\nüìä Data Distribution:\")\n",
        "    print(f\"\\nBy Dataset:\")\n",
        "    print(metadata_df['dataset'].value_counts().to_string())\n",
        "    print(f\"\\nBy Split (Label):\")\n",
        "    print(metadata_df['split'].value_counts().to_string())\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No embedding files found!\")\n",
        "    print(\"   Expected directory structure:\")\n",
        "    print(\"   embeddings/\")\n",
        "    print(\"   ‚îú‚îÄ‚îÄ v1/\")\n",
        "    print(\"   ‚îÇ   ‚îú‚îÄ‚îÄ chat/\")\n",
        "    print(\"   ‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ v1-chat-00000-of-00001.parquet\")\n",
        "    print(\"   ‚îÇ   ‚îî‚îÄ‚îÄ ...\")\n",
        "    print(\"   ‚îî‚îÄ‚îÄ ...\")\n",
        "    print(\"\\n   Run: python extract_embeddings_parallel_shards.py --all\")\n",
        "    embeddings = None\n",
        "    metadata_df = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ GPU-Accelerated Dimensionality Reduction\n",
        "\n",
        "Apply **cuML UMAP** or **cuML t-SNE** for GPU-accelerated dimensionality reduction.\n",
        "This is 10-100x faster than CPU-based methods for large datasets.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "def apply_umap_gpu(embeddings: np.ndarray, n_components: int = 2) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply GPU-accelerated UMAP using cuML.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: Input embeddings (n_samples, n_features)\n",
        "        n_components: Output dimensions (2 or 3)\n",
        "    \n",
        "    Returns:\n",
        "        Reduced embeddings (n_samples, n_components)\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Applying cuML UMAP (GPU-accelerated)...\")\n",
        "    print(f\"   Input shape: {embeddings.shape}\")\n",
        "    print(f\"   Output dimensions: {n_components}\")\n",
        "    print(f\"   Parameters: n_neighbors={UMAP_N_NEIGHBORS}, min_dist={UMAP_MIN_DIST}, metric={UMAP_METRIC}\")\n",
        "    \n",
        "    # Convert to cupy array for GPU processing\n",
        "    embeddings_gpu = cp.asarray(embeddings, dtype=cp.float32)\n",
        "    \n",
        "    # Initialize cuML UMAP\n",
        "    reducer = cumlUMAP(\n",
        "        n_components=n_components,\n",
        "        n_neighbors=UMAP_N_NEIGHBORS,\n",
        "        min_dist=UMAP_MIN_DIST,\n",
        "        metric=UMAP_METRIC,\n",
        "        random_state=RANDOM_SEED,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Fit and transform\n",
        "    reduced = reducer.fit_transform(embeddings_gpu)\n",
        "    \n",
        "    # Convert back to numpy\n",
        "    result = cp.asnumpy(reduced)\n",
        "    \n",
        "    print(f\"‚úÖ UMAP complete! Output shape: {result.shape}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def apply_tsne_gpu(embeddings: np.ndarray, n_components: int = 2) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply GPU-accelerated t-SNE using cuML.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: Input embeddings (n_samples, n_features)\n",
        "        n_components: Output dimensions (2 or 3)\n",
        "    \n",
        "    Returns:\n",
        "        Reduced embeddings (n_samples, n_components)\n",
        "    \"\"\"\n",
        "    print(f\"üöÄ Applying cuML t-SNE (GPU-accelerated)...\")\n",
        "    print(f\"   Input shape: {embeddings.shape}\")\n",
        "    print(f\"   Output dimensions: {n_components}\")\n",
        "    print(f\"   Parameters: perplexity={TSNE_PERPLEXITY}, learning_rate={TSNE_LEARNING_RATE}\")\n",
        "    \n",
        "    # Convert to cupy array for GPU processing\n",
        "    embeddings_gpu = cp.asarray(embeddings, dtype=cp.float32)\n",
        "    \n",
        "    # Initialize cuML t-SNE\n",
        "    reducer = cumlTSNE(\n",
        "        n_components=n_components,\n",
        "        perplexity=TSNE_PERPLEXITY,\n",
        "        learning_rate=TSNE_LEARNING_RATE,\n",
        "        random_state=RANDOM_SEED,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    # Fit and transform\n",
        "    reduced = reducer.fit_transform(embeddings_gpu)\n",
        "    \n",
        "    # Convert back to numpy\n",
        "    result = cp.asnumpy(reduced)\n",
        "    \n",
        "    print(f\"‚úÖ t-SNE complete! Output shape: {result.shape}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def apply_umap_cpu(embeddings: np.ndarray, n_components: int = 2) -> np.ndarray:\n",
        "    \"\"\"CPU fallback for UMAP using umap-learn.\"\"\"\n",
        "    print(f\"üê¢ Applying CPU UMAP (umap-learn)...\")\n",
        "    print(f\"   Input shape: {embeddings.shape}\")\n",
        "    \n",
        "    reducer = cpuUMAP.UMAP(\n",
        "        n_components=n_components,\n",
        "        n_neighbors=UMAP_N_NEIGHBORS,\n",
        "        min_dist=UMAP_MIN_DIST,\n",
        "        metric=UMAP_METRIC,\n",
        "        random_state=RANDOM_SEED,\n",
        "        verbose=True\n",
        "    )\n",
        "    \n",
        "    result = reducer.fit_transform(embeddings)\n",
        "    print(f\"‚úÖ UMAP complete! Output shape: {result.shape}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def apply_tsne_cpu(embeddings: np.ndarray, n_components: int = 2) -> np.ndarray:\n",
        "    \"\"\"CPU fallback for t-SNE using sklearn.\"\"\"\n",
        "    print(f\"üê¢ Applying CPU t-SNE (sklearn)...\")\n",
        "    print(f\"   Input shape: {embeddings.shape}\")\n",
        "    \n",
        "    reducer = sklearnTSNE(\n",
        "        n_components=n_components,\n",
        "        perplexity=TSNE_PERPLEXITY,\n",
        "        learning_rate=TSNE_LEARNING_RATE,\n",
        "        random_state=RANDOM_SEED,\n",
        "        verbose=1\n",
        "    )\n",
        "    \n",
        "    result = reducer.fit_transform(embeddings)\n",
        "    print(f\"‚úÖ t-SNE complete! Output shape: {result.shape}\")\n",
        "    return result\n",
        "\n",
        "\n",
        "def reduce_dimensions(\n",
        "    embeddings: np.ndarray,\n",
        "    method: str = \"umap\",\n",
        "    n_components: int = 2\n",
        ") -> np.ndarray:\n",
        "    \"\"\"\n",
        "    Apply dimensionality reduction using GPU if available, else CPU.\n",
        "    \n",
        "    Args:\n",
        "        embeddings: Input embeddings\n",
        "        method: \"umap\" or \"tsne\"\n",
        "        n_components: 2 or 3\n",
        "    \n",
        "    Returns:\n",
        "        Reduced embeddings\n",
        "    \"\"\"\n",
        "    if RAPIDS_AVAILABLE:\n",
        "        if method == \"umap\":\n",
        "            return apply_umap_gpu(embeddings, n_components)\n",
        "        else:\n",
        "            return apply_tsne_gpu(embeddings, n_components)\n",
        "    else:\n",
        "        if method == \"umap\":\n",
        "            return apply_umap_cpu(embeddings, n_components)\n",
        "        else:\n",
        "            return apply_tsne_cpu(embeddings, n_components)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üó∫Ô∏è  Applying UMAP Dimensionality Reduction\n",
            "================================================================================\n",
            "\n",
            "üìä 2D Projection:\n",
            "üöÄ Applying cuML UMAP (GPU-accelerated)...\n",
            "   Input shape: (1397187, 4096)\n",
            "   Output dimensions: 2\n",
            "   Parameters: n_neighbors=15, min_dist=0.1, metric=cosine\n",
            "[2025-12-23 10:55:17.784] [CUML] [debug] Computing KNN Graph\n"
          ]
        }
      ],
      "source": [
        "# Apply dimensionality reduction (2D and 3D)\n",
        "if embeddings is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"üó∫Ô∏è  Applying {REDUCTION_METHOD.upper()} Dimensionality Reduction\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 2D reduction\n",
        "    print(\"\\nüìä 2D Projection:\")\n",
        "    embeddings_2d = reduce_dimensions(embeddings, method=REDUCTION_METHOD, n_components=2)\n",
        "    \n",
        "    # Add to metadata\n",
        "    metadata_df['x'] = embeddings_2d[:, 0]\n",
        "    metadata_df['y'] = embeddings_2d[:, 1]\n",
        "    \n",
        "    # 3D reduction\n",
        "    print(\"\\nüìä 3D Projection:\")\n",
        "    embeddings_3d = reduce_dimensions(embeddings, method=REDUCTION_METHOD, n_components=3)\n",
        "    \n",
        "    # Add to metadata\n",
        "    metadata_df['x3d'] = embeddings_3d[:, 0]\n",
        "    metadata_df['y3d'] = embeddings_3d[:, 1]\n",
        "    metadata_df['z3d'] = embeddings_3d[:, 2]\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ Dimensionality reduction complete!\")\n",
        "    print(f\"   2D range: x=[{metadata_df['x'].min():.2f}, {metadata_df['x'].max():.2f}], y=[{metadata_df['y'].min():.2f}, {metadata_df['y'].max():.2f}]\")\n",
        "    print(f\"   3D range: x=[{metadata_df['x3d'].min():.2f}, {metadata_df['x3d'].max():.2f}], y=[{metadata_df['y3d'].min():.2f}, {metadata_df['y3d'].max():.2f}], z=[{metadata_df['z3d'].min():.2f}, {metadata_df['z3d'].max():.2f}]\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No embeddings loaded - skipping dimensionality reduction\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Interactive Plotly Visualizations\n",
        "\n",
        "Create 2D and 3D interactive visualizations with Plotly.\n",
        "Labels come from dataset metadata (split names). Future versions can use cuBERT clustering.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def create_2d_scatter(\n",
        "    df: pd.DataFrame,\n",
        "    color_col: str = 'split',\n",
        "    title: str = \"2D Embedding Visualization\",\n",
        "    color_map: Optional[Dict] = None\n",
        ") -> go.Figure:\n",
        "    \"\"\"\n",
        "    Create interactive 2D scatter plot with Plotly.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with x, y columns and metadata\n",
        "        color_col: Column to use for coloring points\n",
        "        title: Plot title\n",
        "        color_map: Optional custom color mapping\n",
        "    \n",
        "    Returns:\n",
        "        Plotly Figure object\n",
        "    \"\"\"\n",
        "    # Use custom colors if provided\n",
        "    if color_map is None:\n",
        "        color_map = CATEGORY_COLORS\n",
        "    \n",
        "    # Get unique values and assign colors\n",
        "    unique_vals = df[color_col].unique()\n",
        "    colors = {val: color_map.get(val, '#95A5A6') for val in unique_vals}\n",
        "    \n",
        "    fig = px.scatter(\n",
        "        df,\n",
        "        x='x',\n",
        "        y='y',\n",
        "        color=color_col,\n",
        "        color_discrete_map=colors,\n",
        "        hover_data=['dataset', 'split', 'label'],\n",
        "        title=title,\n",
        "        labels={'x': f'{REDUCTION_METHOD.upper()} Dimension 1', 'y': f'{REDUCTION_METHOD.upper()} Dimension 2'},\n",
        "        template='plotly_white'\n",
        "    )\n",
        "    \n",
        "    fig.update_traces(\n",
        "        marker=dict(size=5, opacity=0.6, line=dict(width=0.3, color='white'))\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        width=1200,\n",
        "        height=800,\n",
        "        title_font_size=20,\n",
        "        title_x=0.5,\n",
        "        legend=dict(\n",
        "            title=color_col.title(),\n",
        "            yanchor=\"top\",\n",
        "            y=0.99,\n",
        "            xanchor=\"left\",\n",
        "            x=1.01,\n",
        "            bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "            bordercolor=\"gray\",\n",
        "            borderwidth=1\n",
        "        ),\n",
        "        hovermode='closest'\n",
        "    )\n",
        "    \n",
        "    return fig\n",
        "\n",
        "\n",
        "def create_3d_scatter(\n",
        "    df: pd.DataFrame,\n",
        "    color_col: str = 'split',\n",
        "    title: str = \"3D Embedding Visualization\",\n",
        "    color_map: Optional[Dict] = None\n",
        ") -> go.Figure:\n",
        "    \"\"\"\n",
        "    Create interactive 3D scatter plot with Plotly.\n",
        "    \n",
        "    Args:\n",
        "        df: DataFrame with x3d, y3d, z3d columns and metadata\n",
        "        color_col: Column to use for coloring points\n",
        "        title: Plot title\n",
        "        color_map: Optional custom color mapping\n",
        "    \n",
        "    Returns:\n",
        "        Plotly Figure object\n",
        "    \"\"\"\n",
        "    # Use custom colors if provided\n",
        "    if color_map is None:\n",
        "        color_map = CATEGORY_COLORS\n",
        "    \n",
        "    # Get unique values and assign colors\n",
        "    unique_vals = df[color_col].unique()\n",
        "    colors = {val: color_map.get(val, '#95A5A6') for val in unique_vals}\n",
        "    \n",
        "    fig = px.scatter_3d(\n",
        "        df,\n",
        "        x='x3d',\n",
        "        y='y3d',\n",
        "        z='z3d',\n",
        "        color=color_col,\n",
        "        color_discrete_map=colors,\n",
        "        hover_data=['dataset', 'split', 'label'],\n",
        "        title=title,\n",
        "        labels={\n",
        "            'x3d': f'{REDUCTION_METHOD.upper()} Dim 1',\n",
        "            'y3d': f'{REDUCTION_METHOD.upper()} Dim 2',\n",
        "            'z3d': f'{REDUCTION_METHOD.upper()} Dim 3'\n",
        "        },\n",
        "        template='plotly_white'\n",
        "    )\n",
        "    \n",
        "    fig.update_traces(\n",
        "        marker=dict(size=3, opacity=0.6, line=dict(width=0.2, color='white'))\n",
        "    )\n",
        "    \n",
        "    fig.update_layout(\n",
        "        width=1200,\n",
        "        height=900,\n",
        "        title_font_size=20,\n",
        "        title_x=0.5,\n",
        "        scene=dict(\n",
        "            xaxis_title=f'{REDUCTION_METHOD.upper()} Dimension 1',\n",
        "            yaxis_title=f'{REDUCTION_METHOD.upper()} Dimension 2',\n",
        "            zaxis_title=f'{REDUCTION_METHOD.upper()} Dimension 3',\n",
        "            camera=dict(eye=dict(x=1.5, y=1.5, z=1.2))\n",
        "        ),\n",
        "        legend=dict(\n",
        "            title=color_col.title(),\n",
        "            yanchor=\"top\",\n",
        "            y=0.99,\n",
        "            xanchor=\"left\",\n",
        "            x=0.01,\n",
        "            bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "            bordercolor=\"gray\",\n",
        "            borderwidth=1\n",
        "        )\n",
        "    )\n",
        "    \n",
        "    return fig\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create and display visualizations\n",
        "if metadata_df is not None and 'x' in metadata_df.columns:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìä Creating Interactive Visualizations\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # =========================================================================\n",
        "    # Visualization 1: 2D scatter by Split (default label)\n",
        "    # =========================================================================\n",
        "    print(\"\\nüé® Creating 2D visualization colored by Split...\")\n",
        "    fig_2d_split = create_2d_scatter(\n",
        "        metadata_df,\n",
        "        color_col='split',\n",
        "        title=f'{REDUCTION_METHOD.upper()} 2D Projection - Colored by Split'\n",
        "    )\n",
        "    \n",
        "    # Save to HTML\n",
        "    output_file = OUTPUT_DIR / f'{REDUCTION_METHOD}_2d_by_split.html'\n",
        "    fig_2d_split.write_html(str(output_file))\n",
        "    print(f\"   ‚úÖ Saved: {output_file}\")\n",
        "    \n",
        "    # Display\n",
        "    fig_2d_split.show()\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No data to visualize - run previous cells first\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================================\n",
        "# Visualization 2: 2D scatter by Dataset\n",
        "# =========================================================================\n",
        "if metadata_df is not None and 'x' in metadata_df.columns:\n",
        "    print(\"\\nüé® Creating 2D visualization colored by Dataset...\")\n",
        "    \n",
        "    # Custom colors for datasets\n",
        "    dataset_colors = {\n",
        "        'v1': '#FF6B6B',\n",
        "        'v2': '#4ECDC4',\n",
        "        'llama-sft': '#45B7D1',\n",
        "        'llama-rl': '#96CEB4',\n",
        "        'v3-science': '#DDA0DD',\n",
        "        'v3-math-proofs': '#F39C12',\n",
        "        'v3-instruction-chat': '#9B59B6'\n",
        "    }\n",
        "    \n",
        "    fig_2d_dataset = create_2d_scatter(\n",
        "        metadata_df,\n",
        "        color_col='dataset',\n",
        "        title=f'{REDUCTION_METHOD.upper()} 2D Projection - Colored by Dataset',\n",
        "        color_map=dataset_colors\n",
        "    )\n",
        "    \n",
        "    # Save to HTML\n",
        "    output_file = OUTPUT_DIR / f'{REDUCTION_METHOD}_2d_by_dataset.html'\n",
        "    fig_2d_dataset.write_html(str(output_file))\n",
        "    print(f\"   ‚úÖ Saved: {output_file}\")\n",
        "    \n",
        "    # Display\n",
        "    fig_2d_dataset.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================================\n",
        "# Visualization 3: 3D scatter by Split\n",
        "# =========================================================================\n",
        "if metadata_df is not None and 'x3d' in metadata_df.columns:\n",
        "    print(\"\\nüé® Creating 3D visualization colored by Split...\")\n",
        "    fig_3d_split = create_3d_scatter(\n",
        "        metadata_df,\n",
        "        color_col='split',\n",
        "        title=f'{REDUCTION_METHOD.upper()} 3D Projection - Colored by Split'\n",
        "    )\n",
        "    \n",
        "    # Save to HTML\n",
        "    output_file = OUTPUT_DIR / f'{REDUCTION_METHOD}_3d_by_split.html'\n",
        "    fig_3d_split.write_html(str(output_file))\n",
        "    print(f\"   ‚úÖ Saved: {output_file}\")\n",
        "    \n",
        "    # Display\n",
        "    fig_3d_split.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# =========================================================================\n",
        "# Visualization 4: 3D scatter by Dataset\n",
        "# =========================================================================\n",
        "if metadata_df is not None and 'x3d' in metadata_df.columns:\n",
        "    print(\"\\nüé® Creating 3D visualization colored by Dataset...\")\n",
        "    fig_3d_dataset = create_3d_scatter(\n",
        "        metadata_df,\n",
        "        color_col='dataset',\n",
        "        title=f'{REDUCTION_METHOD.upper()} 3D Projection - Colored by Dataset',\n",
        "        color_map=dataset_colors\n",
        "    )\n",
        "    \n",
        "    # Save to HTML\n",
        "    output_file = OUTPUT_DIR / f'{REDUCTION_METHOD}_3d_by_dataset.html'\n",
        "    fig_3d_dataset.write_html(str(output_file))\n",
        "    print(f\"   ‚úÖ Saved: {output_file}\")\n",
        "    \n",
        "    # Display\n",
        "    fig_3d_dataset.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "## üìà Summary and Export\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary\n",
        "if metadata_df is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üìä VISUALIZATION SUMMARY\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    print(f\"\\n‚úÖ Total embeddings visualized: {len(metadata_df):,}\")\n",
        "    print(f\"   Embedding dimension: {embeddings.shape[1] if embeddings is not None else 'N/A'}\")\n",
        "    print(f\"   Reduction method: {REDUCTION_METHOD.upper()}\")\n",
        "    print(f\"   Backend: {'GPU (RAPIDS cuML)' if RAPIDS_AVAILABLE else 'CPU'}\")\n",
        "    \n",
        "    print(f\"\\nüìÅ Output files saved to: {OUTPUT_DIR}\")\n",
        "    for f in OUTPUT_DIR.glob(f\"{REDUCTION_METHOD}_*.html\"):\n",
        "        print(f\"   ‚Ä¢ {f.name}\")\n",
        "    \n",
        "    print(f\"\\nüìä Data Distribution:\")\n",
        "    print(f\"\\nBy Dataset:\")\n",
        "    print(metadata_df['dataset'].value_counts().to_string())\n",
        "    print(f\"\\nBy Split:\")\n",
        "    print(metadata_df['split'].value_counts().to_string())\n",
        "    \n",
        "    # Save metadata DataFrame for further analysis\n",
        "    metadata_file = OUTPUT_DIR / 'visualization_metadata.parquet'\n",
        "    metadata_df.to_parquet(str(metadata_file))\n",
        "    print(f\"\\nüíæ Metadata saved to: {metadata_file}\")\n",
        "    \n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"‚úÖ Visualization complete!\")\n",
        "    print(\"   Open the HTML files in a browser for interactive exploration.\")\n",
        "    print(\"=\" * 80)\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No visualizations created - no embeddings loaded\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÑ Optional: Compare UMAP vs t-SNE\n",
        "\n",
        "Run this cell to also generate t-SNE visualizations for comparison.\n",
        "Note: t-SNE is typically slower than UMAP, even with GPU acceleration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optional: Generate t-SNE visualizations\n",
        "# Set RUN_TSNE = True to generate t-SNE visualizations\n",
        "RUN_TSNE = False\n",
        "\n",
        "if RUN_TSNE and embeddings is not None:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"üîÑ Generating t-SNE visualizations for comparison...\")\n",
        "    print(\"=\" * 80)\n",
        "    \n",
        "    # 2D t-SNE\n",
        "    print(\"\\nüìä 2D t-SNE:\")\n",
        "    tsne_2d = reduce_dimensions(embeddings, method=\"tsne\", n_components=2)\n",
        "    metadata_df['tsne_x'] = tsne_2d[:, 0]\n",
        "    metadata_df['tsne_y'] = tsne_2d[:, 1]\n",
        "    \n",
        "    # Create 2D t-SNE plot\n",
        "    fig_tsne_2d = px.scatter(\n",
        "        metadata_df,\n",
        "        x='tsne_x',\n",
        "        y='tsne_y',\n",
        "        color='split',\n",
        "        color_discrete_map=CATEGORY_COLORS,\n",
        "        hover_data=['dataset', 'split'],\n",
        "        title='t-SNE 2D Projection - Colored by Split',\n",
        "        template='plotly_white'\n",
        "    )\n",
        "    fig_tsne_2d.update_traces(marker=dict(size=5, opacity=0.6))\n",
        "    fig_tsne_2d.update_layout(width=1200, height=800)\n",
        "    \n",
        "    output_file = OUTPUT_DIR / 'tsne_2d_by_split.html'\n",
        "    fig_tsne_2d.write_html(str(output_file))\n",
        "    print(f\"   ‚úÖ Saved: {output_file}\")\n",
        "    fig_tsne_2d.show()\n",
        "    \n",
        "    print(\"\\n‚úÖ t-SNE visualizations complete!\")\n",
        "elif RUN_TSNE:\n",
        "    print(\"‚ö†Ô∏è  No embeddings available for t-SNE\")\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  t-SNE comparison skipped (set RUN_TSNE = True to enable)\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîÆ Future Work: Label Assignment\n",
        "\n",
        "Labels can come from multiple sources:\n",
        "\n",
        "1. **Dataset Metadata** (current): Using `split` names (chat, code, math, stem, etc.)\n",
        "2. **cuBERT Clustering**: GPU-accelerated topic modeling with BERT embeddings\n",
        "3. **K-Means/HDBSCAN**: Unsupervised clustering on the reduced embeddings\n",
        "4. **Manual Labels**: Domain expert annotations\n",
        "\n",
        "To add cuBERT-based labels, see: [RAPIDS cuBERT Topic Modelling](https://github.com/rapidsai/rapids-examples/tree/main/cuBERT_topic_modelling)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Adding cluster labels with cuML HDBSCAN (optional)\n",
        "ADD_CLUSTER_LABELS = False\n",
        "\n",
        "if ADD_CLUSTER_LABELS and RAPIDS_AVAILABLE and embeddings is not None:\n",
        "    from cuml.cluster import HDBSCAN\n",
        "    \n",
        "    print(\"üîç Computing HDBSCAN clusters on GPU...\")\n",
        "    \n",
        "    # Use 2D reduced embeddings for clustering\n",
        "    embeddings_for_clustering = cp.asarray(embeddings_2d, dtype=cp.float32)\n",
        "    \n",
        "    clusterer = HDBSCAN(\n",
        "        min_cluster_size=50,\n",
        "        min_samples=10,\n",
        "        metric='euclidean'\n",
        "    )\n",
        "    \n",
        "    cluster_labels = clusterer.fit_predict(embeddings_for_clustering)\n",
        "    metadata_df['cluster'] = cp.asnumpy(cluster_labels)\n",
        "    \n",
        "    n_clusters = len(set(metadata_df['cluster'])) - (1 if -1 in metadata_df['cluster'].values else 0)\n",
        "    print(f\"   Found {n_clusters} clusters\")\n",
        "    \n",
        "    # Visualize clusters\n",
        "    fig_clusters = px.scatter(\n",
        "        metadata_df,\n",
        "        x='x', y='y',\n",
        "        color='cluster',\n",
        "        title=f'{REDUCTION_METHOD.upper()} with HDBSCAN Clusters ({n_clusters} clusters)',\n",
        "        template='plotly_white'\n",
        "    )\n",
        "    fig_clusters.update_layout(width=1200, height=800)\n",
        "    fig_clusters.show()\n",
        "else:\n",
        "    print(\"‚ÑπÔ∏è  Cluster labeling skipped (set ADD_CLUSTER_LABELS = True to enable)\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# End of notebook\n",
        "print(\"üéâ Notebook execution complete!\")\n",
        "print(\"\\nNext steps:\")\n",
        "print(\"1. Open the HTML files in visualizations/ folder for interactive exploration\")\n",
        "print(\"2. Set ADD_CLUSTER_LABELS = True to compute unsupervised clusters\")\n",
        "print(\"3. Set RUN_TSNE = True to compare UMAP vs t-SNE\")\n",
        "print(\"4. Integrate cuBERT for topic-based labeling\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Scratch cell for experimentation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Additional scratch space\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Empty cell\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# End\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nemotron",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
