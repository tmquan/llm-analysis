{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# NVIDIA Nemotron Datasets Visualization with UMAP\n",
        "\n",
        "This notebook processes Nemotron-v1 and Nemotron-v2 datasets in parallel, generates embeddings using NVIDIA's NIM (nv-embedqa-e5-v5), and visualizes the data using UMAP 2D projection with **interactive Plotly visualizations**.\n",
        "\n",
        "## üìä Overview\n",
        "- **Datasets**: Nemotron-Post-Training-Dataset-v1 and v2\n",
        "- **Embedding Model**: nvidia/nv-embedqa-e5-v5 (NVIDIA NIM)\n",
        "- **Visualization**: Interactive UMAP 2D/3D plots with Plotly\n",
        "- **Color Coding**: Based on dataset headers (category, reasoning, version, split)\n",
        "\n",
        "## üéØ Features\n",
        "- ‚úÖ Parallel processing for embedding generation\n",
        "- ‚úÖ NVIDIA NIM API integration for high-quality embeddings\n",
        "- ‚úÖ Interactive Plotly visualizations with hover details\n",
        "- ‚úÖ Multiple views: by category, version, reasoning, and faceted comparisons\n",
        "- ‚úÖ 3D visualization for deeper exploration\n",
        "- ‚úÖ Export to HTML for easy sharing\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîß Setup and Imports\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "# Install required packages\n",
        "%pip install dotenv datasets umap-learn numpy pandas plotly scikit-learn openai tqdm joblib nbformat>=5.0.0 -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ All imports successful!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import plotly.express as px\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "from datasets import load_dataset\n",
        "from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor, as_completed\n",
        "from tqdm.auto import tqdm\n",
        "import pickle\n",
        "import json\n",
        "from openai import OpenAI\n",
        "import umap\n",
        "from typing import List, Dict, Tuple\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Configure Plotly for notebook rendering\n",
        "import plotly.io as pio\n",
        "pio.renderers.default = \"notebook_connected\"\n",
        "\n",
        "print(\"‚úÖ All imports successful!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì¶ Configure NVIDIA NIM Client\n",
        "\n",
        "Set up the OpenAI client to connect to NVIDIA's NIM endpoint for embeddings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dotenv import load_dotenv\n",
        "load_dotenv()\n",
        "\n",
        "# Configure NVIDIA NIM client\n",
        "nvidia_api_key = os.environ.get(\"NVIDIA_API_KEY\", \"nvapi-YOUR_API_KEY\")\n",
        "# print(nvidia_api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ NVIDIA NIM client configured with model: nvidia/llama-3_2-nemoretriever-300m-embed-v2\n",
            "   Base URL: https://integrate.api.nvidia.com/v1\n"
          ]
        }
      ],
      "source": [
        "# Model configuration\n",
        "# EMBEDDING_MODEL = \"nvidia/nv-embedqa-e5-v5\"\n",
        "# EMBEDDING_DIM = 1024  # Expected dimension for this model\n",
        "EMBEDDING_MODEL = \"nvidia/llama-3_2-nemoretriever-300m-embed-v2\"\n",
        "EMBEDDING_DIM = 4096  # Expected dimension for this model\n",
        "BASE_URL = \"https://integrate.api.nvidia.com/v1\"    \n",
        "print(f\"‚úÖ NVIDIA NIM client configured with model: {EMBEDDING_MODEL}\")\n",
        "print(f\"   Base URL: {BASE_URL}\")\n",
        "\n",
        "client = OpenAI(\n",
        "    base_url=BASE_URL,\n",
        "    api_key=nvidia_api_key\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üß™ Test Embedding API Payload\n",
        "\n",
        "Before running the full extraction, let's test the embedding creation with random sample data to verify:\n",
        "1. The API endpoint is working correctly\n",
        "2. The request payload format is accepted\n",
        "3. The response format is as expected\n",
        "4. The embedding dimensions are correct\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üß™ Testing Embedding API with Random Sample Data\n",
            "================================================================================\n",
            "\n",
            "üìä Test Configuration:\n",
            "   Model: nvidia/llama-3_2-nemoretriever-300m-embed-v2\n",
            "   Number of test samples: 5\n",
            "   Expected dimension: 4096\n",
            "\n",
            "üìù Sample texts (first 80 chars):\n",
            "   1. user: Write a Python function to calculate fibonacci numbers assistant: Here's a...\n",
            "   2. user: Explain quantum entanglement in simple terms assistant: Quantum entangleme...\n",
            "   3. user: How do I solve quadratic equations? assistant: To solve ax¬≤ + bx + c = 0, ...\n",
            "   4. user: What are best practices for REST API design? assistant: RESTful API design...\n",
            "   5. system: You are a helpful assistant user: Tell me a joke assistant: Why don't sc...\n",
            "\n",
            "================================================================================\n",
            "üîç Testing SINGLE text embedding...\n",
            "================================================================================\n",
            "‚ùå Single text embedding failed!\n",
            "   Error: 404 page not found\n",
            "   Error type: NotFoundError\n",
            "   Response status: 404\n",
            "\n",
            "================================================================================\n",
            "üîç Testing BATCH embedding (all 5 samples)...\n",
            "================================================================================\n",
            "‚ùå Batch embedding failed!\n",
            "   Error: 404 page not found\n",
            "   Error type: NotFoundError\n",
            "   Response status: 404\n",
            "\n",
            "================================================================================\n",
            "üß™ Embedding API test complete!\n",
            "================================================================================\n"
          ]
        }
      ],
      "source": [
        "import json\n",
        "import random\n",
        "\n",
        "print(\"üß™ Testing Embedding API with Random Sample Data\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Generate random test data to simulate different types of content\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "# Sample texts simulating different categories\n",
        "sample_texts = [\n",
        "    \"user: Write a Python function to calculate fibonacci numbers\\nassistant: Here's a recursive implementation of fibonacci...\",\n",
        "    \"user: Explain quantum entanglement in simple terms\\nassistant: Quantum entanglement is a phenomenon where particles become connected...\",\n",
        "    \"user: How do I solve quadratic equations?\\nassistant: To solve ax¬≤ + bx + c = 0, you can use the quadratic formula...\",\n",
        "    \"user: What are best practices for REST API design?\\nassistant: RESTful API design should follow these principles...\",\n",
        "    \"system: You are a helpful assistant\\nuser: Tell me a joke\\nassistant: Why don't scientists trust atoms? Because they make up everything!\"\n",
        "]\n",
        "\n",
        "# Create random test dataframe\n",
        "test_df = pd.DataFrame({\n",
        "    'text': sample_texts,\n",
        "    'version': ['v1', 'v1', 'v2', 'v2', 'v1'],\n",
        "    'split': ['code', 'stem', 'math', 'chat', 'chat'],\n",
        "    'category': ['code', 'stem', 'math', 'chat', 'chat'],\n",
        "    'reasoning': ['on', 'on', 'off', 'on', 'off'],\n",
        "    'idx': [0, 1, 2, 3, 4]\n",
        "})\n",
        "\n",
        "print(f\"\\nüìä Test Configuration:\")\n",
        "print(f\"   Model: {EMBEDDING_MODEL}\")\n",
        "print(f\"   Number of test samples: {len(test_df)}\")\n",
        "print(f\"   Expected dimension: {EMBEDDING_DIM}\")\n",
        "\n",
        "# print(f\"\\nüìù Random test dataframe:\")\n",
        "# print(test_df[['category', 'version', 'reasoning']].to_string())\n",
        "\n",
        "print(f\"\\nüìù Sample texts (first 80 chars):\")\n",
        "for i, text in enumerate(test_df['text'].tolist(), 1):\n",
        "    preview = text.replace('\\n', ' ')[:80]\n",
        "    print(f\"   {i}. {preview}...\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üîç Testing SINGLE text embedding...\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "test_texts = test_df['text'].tolist()\n",
        "\n",
        "try:\n",
        "    # Test with single text first\n",
        "    response = client.embeddings.create(\n",
        "        input=[test_texts[0]],\n",
        "        model=EMBEDDING_MODEL,\n",
        "        encoding_format=\"float\",\n",
        "        extra_body={\"input_type\": \"query\", \"truncate\": \"NONE\"}\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Single text embedding successful!\")\n",
        "    print(f\"   Response type: {type(response)}\")\n",
        "    print(f\"   Number of embeddings: {len(response.data)}\")\n",
        "    print(f\"   Embedding dimension: {len(response.data[0].embedding)}\")\n",
        "    print(f\"   First 10 values: {response.data[0].embedding[:10]}\")\n",
        "    \n",
        "    # Verify dimension\n",
        "    actual_dim = len(response.data[0].embedding)\n",
        "    if actual_dim == EMBEDDING_DIM:\n",
        "        print(f\"   ‚úÖ Dimension matches expected: {actual_dim} == {EMBEDDING_DIM}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Dimension mismatch: {actual_dim} != {EMBEDDING_DIM}\")\n",
        "        print(f\"      Updating EMBEDDING_DIM to {actual_dim}\")\n",
        "        EMBEDDING_DIM = actual_dim\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Single text embedding failed!\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    print(f\"   Error type: {type(e).__name__}\")\n",
        "    if hasattr(e, 'response'):\n",
        "        print(f\"   Response status: {getattr(e.response, 'status_code', 'N/A')}\")\n",
        "\n",
        "print(f\"\\n{'='*80}\")\n",
        "print(f\"üîç Testing BATCH embedding (all {len(test_texts)} samples)...\")\n",
        "print(f\"{'='*80}\")\n",
        "\n",
        "try:\n",
        "    # Test with batch\n",
        "    response = client.embeddings.create(\n",
        "        input=test_texts,\n",
        "        model=EMBEDDING_MODEL,\n",
        "        encoding_format=\"float\",\n",
        "        extra_body={\"input_type\": \"passage\", \"truncate\": \"NONE\"}\n",
        "    )\n",
        "    \n",
        "    print(\"‚úÖ Batch embedding successful!\")\n",
        "    print(f\"   Response type: {type(response)}\")\n",
        "    print(f\"   Number of embeddings returned: {len(response.data)}\")\n",
        "    print(f\"   Expected number: {len(test_texts)}\")\n",
        "    print(f\"   All dimensions: {[len(d.embedding) for d in response.data]}\")\n",
        "    \n",
        "    # Verify all dimensions match\n",
        "    dims = [len(d.embedding) for d in response.data]\n",
        "    if len(set(dims)) == 1 and dims[0] == EMBEDDING_DIM:\n",
        "        print(f\"   ‚úÖ All dimensions consistent: {dims[0]}\")\n",
        "    else:\n",
        "        print(f\"   ‚ö†Ô∏è  Dimension inconsistency detected: {set(dims)}\")\n",
        "    \n",
        "    # print(f\"\\nüìä Sample embedding statistics:\")\n",
        "    # sample_embedding = np.array(response.data[0].embedding)\n",
        "    # print(f\"   Mean: {sample_embedding.mean():.6f}\")\n",
        "    # print(f\"   Std: {sample_embedding.std():.6f}\")\n",
        "    # print(f\"   Min: {sample_embedding.min():.6f}\")\n",
        "    # print(f\"   Max: {sample_embedding.max():.6f}\")\n",
        "    # print(f\"   Norm (L2): {np.linalg.norm(sample_embedding):.6f}\")\n",
        "    \n",
        "    # Print first embedding in full detail\n",
        "    print(f\"\\nüîç FULL FIRST TEST EMBEDDING:\")\n",
        "    print(f\"   Text: '{test_texts[0][:80]}...'\")\n",
        "    print(f\"   Dimension: {len(response.data[0].embedding)}\")\n",
        "    print(f\"   Full embedding array:\")\n",
        "    embedding_1 = response.data[0].embedding\n",
        "    # Print in rows of 10 values for readability\n",
        "    for i in range(0, len(embedding_1), 10):\n",
        "        chunk = embedding_1[i:i+10]\n",
        "        values_str = \", \".join([f\"{v:8.5f}\" for v in chunk])\n",
        "        print(f\"      [{i:4d}:{min(i+10, len(embedding_1)):4d}] {values_str}\")\n",
        "    \n",
        "    # Also save embeddings to test_df for verification\n",
        "    test_df['embedding'] = [d.embedding for d in response.data]\n",
        "    print(f\"\\n‚úÖ Saved {len(response.data)} embeddings to test_df['embedding']\")\n",
        "    print(f\"   test_df shape: {test_df.shape}\")\n",
        "    print(f\"   Embedding column type: {type(test_df['embedding'].iloc[0])}\")\n",
        "    print(f\"   First embedding length: {len(test_df['embedding'].iloc[0])}\")\n",
        "    \n",
        "    # Test payload structure\n",
        "    print(f\"\\nüì¶ API Request Payload Structure:\")\n",
        "    print(f\"   ‚úÖ input: list of {len(test_texts)} strings\")\n",
        "    print(f\"   ‚úÖ model: {EMBEDDING_MODEL}\")\n",
        "    print(f\"   ‚úÖ encoding_format: float\")\n",
        "    print(f\"   ‚úÖ extra_body: {{'input_type': 'passage', 'truncate': 'NONE'}}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Batch embedding failed!\")\n",
        "    print(f\"   Error: {e}\")\n",
        "    print(f\"   Error type: {type(e).__name__}\")\n",
        "    \n",
        "    # Try to extract more details\n",
        "    if hasattr(e, 'response'):\n",
        "        print(f\"   Response status: {getattr(e.response, 'status_code', 'N/A')}\")\n",
        "        try:\n",
        "            error_body = e.response.json() if hasattr(e.response, 'json') else str(e.response.text)\n",
        "            print(f\"   Response body: {error_body}\")\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üß™ Embedding API test complete!\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üì• Load Datasets\n",
        "\n",
        "Load both Nemotron v1 and v2 datasets from the local cache.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üì• Loading Nemotron datasets...\n",
            "\n",
            "Loading Nemotron-Post-Training-Dataset-v1...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ba665c0eb8c84152996daaa9133b4620",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f0805f4e897548aaa6f55a9eedb4a8db",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/159 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "92a95884bf0640838fd6abff3f19c615",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/660 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "80b415ee54e64d5c84cdc3544fdfd6a3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/183 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01bb09cc3d484b24afe983623675de9a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/159 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e04c8ac4b7994f0186b7f724f7bfc3c7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/660 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "630ed9918f294d20a08e9deb2c0ebd11",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/175 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3d3e54bf90cb48a99bcf1672b9d036a9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/152 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0253d6e6fe7b4c9aaed493868d787ab7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/649 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Loaded v1 with splits: ['chat', 'code', 'math', 'stem', 'tool_calling']\n",
            "   Total v1 samples: 25,659,642\n",
            "\n",
            "Loading Nemotron-Post-Training-Dataset-v2...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aa9215b5c97b44edb29c1413e9f1e5c2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8c6d8a21fdc04d81a401eaf5a0abb88c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8f0bb557c6b74411ac7f013aeac66baf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ee44f84f9584aeeb0ba86b75caff08b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "94c090dbb59b4ea7bf2b20b08035f607",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9379fe5d686541a8991b8818bb5f5b01",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b7ae969a3ef54bdda92a8bf855ced5a1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e21d0874c13e4a78ba45bdd5cf810f82",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/38 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "49756bb5f0004c54bf8aa8c39fd5b6bd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/33 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4ffe292175eb426c8587f8bc6fc218a6",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Resolving data files:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6852f1c5c8794ae3979183fd18d577c5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "442b36c705fc42369fd8f42cbcb5b67e",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6fdfc5d31c0849f79a5833976f6cb4aa",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/37 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e541200e2781473e9dbc8c09f4605d43",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/32 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "fa4d3571aed54307af25f5cb38d0ff48",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading dataset shards:   0%|          | 0/36 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Loaded v2 with splits: ['stem', 'chat', 'math', 'code', 'multilingual_ja', 'multilingual_de', 'multilingual_it', 'multilingual_es', 'multilingual_fr']\n",
            "   Total v2 samples: 6,341,414\n",
            "\n",
            "‚úÖ All datasets loaded successfully!\n"
          ]
        }
      ],
      "source": [
        "print(\"üì• Loading Nemotron datasets...\\n\")\n",
        "\n",
        "# Load Nemotron v1\n",
        "print(\"Loading Nemotron-Post-Training-Dataset-v1...\")\n",
        "dataset_v1 = load_dataset(\n",
        "    \"nvidia/Nemotron-Post-Training-Dataset-v1\",\n",
        "    cache_dir=\"./datasets/nemotron-v1\"\n",
        ")\n",
        "print(f\"   ‚úÖ Loaded v1 with splits: {list(dataset_v1.keys())}\")\n",
        "print(f\"   Total v1 samples: {sum(len(dataset_v1[split]) for split in dataset_v1.keys()):,}\\n\")\n",
        "\n",
        "# Load Nemotron v2\n",
        "print(\"Loading Nemotron-Post-Training-Dataset-v2...\")\n",
        "dataset_v2 = load_dataset(\n",
        "    \"nvidia/Nemotron-Post-Training-Dataset-v2\",\n",
        "    cache_dir=\"./datasets/nemotron-v2\"\n",
        ")\n",
        "print(f\"   ‚úÖ Loaded v2 with splits: {list(dataset_v2.keys())}\")\n",
        "print(f\"   Total v2 samples: {sum(len(dataset_v2[split]) for split in dataset_v2.keys()):,}\\n\")\n",
        "\n",
        "print(\"‚úÖ All datasets loaded successfully!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üîç Explore Dataset Structure\n",
        "\n",
        "Let's examine the structure of the datasets to understand what fields are available, particularly the most meaning full fields\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "üìä Nemotron v1 Dataset Structure\n",
            "================================================================================\n",
            "\n",
            "Fields in v1: ['uuid', 'license', 'generator', 'version', 'category', 'reasoning', 'messages', 'metadata']\n",
            "\n",
            "uuid: 1b07b912-0135-4f23-b704-2ceea567f617\n",
            "license: CC BY 4.0\n",
            "generator: Qwen3-235B-A22B\n",
            "version: v1\n",
            "category: chat\n",
            "reasoning: off\n",
            "messages: [{'role': 'user', 'content': '', 'tool_calls': []}, {'role': 'assistant', 'content': \"Understood. I'm ready to proceed with the activity. Please ask the first question.\", 'tool_calls': []}]\n",
            "metadata: {\"conversation_id\": \"8e31a022d01d49748f6053a8805dfbd2\", \"source\": \"https://huggingface.co/datasets/lmsys/lmsys-chat-1m\"}\n",
            "\n",
            "================================================================================\n",
            "üìä Nemotron v2 Dataset Structure\n",
            "================================================================================\n",
            "\n",
            "Fields in v2: ['uuid', 'license', 'generator', 'version', 'category', 'reasoning', 'messages']\n",
            "\n",
            "uuid: 76242391-3c82-4471-a971-e51f57b2899e\n",
            "license: CC BY 4.0\n",
            "generator: Qwen3-235B-A22B, Qwen3-30B-A3B\n",
            "version: v2\n",
            "category: chat\n",
            "reasoning: off\n",
            "messages: [{'role': 'system', 'content': ''}, {'role': 'user', 'content': 'Write a description of Mijaƒçija and Brsjaƒçija regions.'}, {'role': 'assistant', 'content': '<think></think>\\n\\n**Mijaƒçija and Brsjaƒçija: A Dualistic Realm of Contrasts**\\n\\nNestled in the heart of a fictional Slavic-inspired world, the regions of **Mijaƒçija** and **Brsjaƒçija** form a strikingly contrasting yet interconnected land steeped in myth, tradition, and natural beauty. These neighboring territories, shaped by their geography and history, embody a duality that defines their cultures, economies, and folklore.\\n\\n---\\n\\n### **Geography and Climate**  \\n- **Mijaƒçija**, often called the \"Emerald Vale,\" is a lush lowland region cradled by gentle rivers and fertile plains. Its name, derived from the endearing term *\"mija\"* (meaning \"dear\" or \"beloved\"), reflects its reputation as a land of abundance. Rolling meadows, dense forests of oak and birch, and sprawling vineyards dominate its landscape. The climate is temperate, with mild, rainy springs and warm summers, ideal for agriculture.  \\n- **Brsjaƒçija**, the \"Highland Bastion,\" rises abruptly to the north, its rugged terrain defined by steep mountains, craggy cliffs, and deep valleys. The name likely stems from *\"brsjak\"* (evoking \"bristle\" or \"roughness\"), a nod to its untamed wilderness. Snow-capped peaks, alpine meadows, and dense fir forests characterize this colder, harsher realm. Glacial rivers carve through its valleys, feeding the lowlands below.\\n\\n---\\n\\n### **Culture and Society**  \\n- **Mijaƒçija** thrives on agrarian traditions. Its people are known for vibrant folk art, intricate embroidery, and a love of music and dance. Villages gather annually for the *\"Zlatna ≈Ωetva\"* (Golden Harvest Festival), celebrating fertility and renewal with bonfires, feasts, and storytelling. The region venerates deities of earth and growth, such as *Zora*, the Dawn Mother.  \\n- **Brsjaƒçija**‚Äôs inhabitants are hardy pastoralists and miners, valuing resilience and self-reliance. Their culture centers on ancestral honor and mountain spirits, particularly *Grom*, the Thunder Lord, believed to dwell in the peaks. Brsjaƒçiyan architecture features stone watchtowers and fortified homesteads, while their oral traditions recount epic tales of dragon-slayers and cursed mines.\\n\\n---\\n\\n### **Economy and Resources**  \\n- **Mijaƒçija** exports grains, wine, and honey, traded via river barges to distant markets. Its forests yield timber and medicinal herbs, while its rivers teem with fish.  \\n- **Brsjaƒçija**‚Äôs wealth lies underground: veins of silver, iron, and the fabled *ƒçernokam* (blackstone), a mystical ore said to hum with latent energy. Herding mountain goats and crafting durable woolen textiles also sustain its economy. Caravans wind down treacherous passes to barter metals and gems for Mijaƒçija‚Äôs grains and wines.\\n\\n---\\n\\n### **History and Conflict**  \\nThe two regions have long shared a fraught alliance. Centuries ago, the **War of Shattered Peaks** erupted when Mijaƒçija‚Äôs rulers sought control of Brsjaƒçija‚Äôs mines, sparking a decade of guerrilla warfare. A fragile peace was sealed by the **Pact of Twin Rivers**, which established shared access to waterways and trade routes. Today, tensions simmer beneath mutual dependence: Mijaƒçija needs Brsjaƒçija‚Äôs metals; Brsjaƒçija relies on Mijaƒçija‚Äôs food surpluses.\\n\\n---\\n\\n### **Legends and Lore**  \\n- Mijaƒçijans whisper of the **Vila Zora**, ethereal forest spirits who bless crops or curse the arrogant. The ruins of *Stari Grad*, a prehistoric hillfort, are said to be her sanctuary.  \\n- In Brsjaƒçija, the **Cursed Mine of Krvava Jama** looms large in folklore‚Äîa labyrinthine pit where a dragon‚Äôs hoard supposedly turned miners to stone. Brave souls still seek its treasures, though none return unchanged.\\n\\n---\\n\\n### **Modern Era**  \\nBoth regions now face external threats: encroaching industrialization from the east and mysterious tremors shaking Brsjaƒçija‚Äôs peaks. Meanwhile, a new generation of Mijaƒçija‚Äôs merchants and Brsjaƒçija‚Äôs warriors debate the old ways, torn between tradition and progress. Yet, the bond of their shared history‚Äîand the inevitable interplay of valley and mountain‚Äîendures.\\n\\n---\\n\\nMijaƒçija and Brsjaƒçija epitomize the dance of opposites: earth and stone, plenty and peril, harmony and strife. Together, they form a realm where myth and reality blur, forever shaping the souls of those who call them home.'}]\n",
            "\n",
            "================================================================================\n",
            "\n",
            "üìä Analyzing categorical fields across all splits...\n",
            "\n",
            "V1 Categories and Reasoning:\n",
            "  chat: categories={'chat'}, reasoning={'off'}\n",
            "  code: categories={'code'}, reasoning={'on'}\n",
            "\n",
            "V2 Categories and Reasoning:\n",
            "  stem: categories={'stem'}, reasoning={'off'}\n",
            "  chat: categories={'chat'}, reasoning={'off'}\n"
          ]
        }
      ],
      "source": [
        "# Examine sample from v1\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä Nemotron v1 Dataset Structure\")\n",
        "print(\"=\" * 80)\n",
        "sample_v1 = dataset_v1['chat'][0]\n",
        "print(f\"\\nFields in v1: {list(sample_v1.keys())}\\n\")\n",
        "for key, value in sample_v1.items():\n",
        "    if isinstance(value, str) and len(value) > 200:\n",
        "        print(f\"{key}: {value[:200]}...\")\n",
        "    elif isinstance(value, list) and len(value) > 3:\n",
        "        print(f\"{key}: {value[:3]}... (truncated)\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "# Examine sample from v2\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä Nemotron v2 Dataset Structure\")\n",
        "print(\"=\" * 80)\n",
        "sample_v2 = dataset_v2['chat'][0]\n",
        "print(f\"\\nFields in v2: {list(sample_v2.keys())}\\n\")\n",
        "for key, value in sample_v2.items():\n",
        "    if isinstance(value, str) and len(value) > 200:\n",
        "        print(f\"{key}: {value[:200]}...\")\n",
        "    elif isinstance(value, list) and len(value) > 3:\n",
        "        print(f\"{key}: {value[:3]}... (truncated)\")\n",
        "    else:\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "\n",
        "# Analyze all unique values for categorical fields\n",
        "print(\"\\nüìä Analyzing categorical fields across all splits...\\n\")\n",
        "\n",
        "# V1 analysis\n",
        "print(\"V1 Categories and Reasoning:\")\n",
        "for split in list(dataset_v1.keys())[:2]:  # Check first 2 splits\n",
        "    categories = set()\n",
        "    reasoning_vals = set()\n",
        "    for i in range(min(100, len(dataset_v1[split]))):\n",
        "        sample = dataset_v1[split][i]\n",
        "        if 'category' in sample:\n",
        "            categories.add(sample['category'])\n",
        "        if 'reasoning' in sample:\n",
        "            reasoning_vals.add(sample['reasoning'])\n",
        "    print(f\"  {split}: categories={categories}, reasoning={reasoning_vals}\")\n",
        "\n",
        "print(\"\\nV2 Categories and Reasoning:\")\n",
        "for split in list(dataset_v2.keys())[:2]:  # Check first 2 splits\n",
        "    categories = set()\n",
        "    reasoning_vals = set()\n",
        "    for i in range(min(100, len(dataset_v2[split]))):\n",
        "        sample = dataset_v2[split][i]\n",
        "        if 'category' in sample:\n",
        "            categories.add(sample['category'])\n",
        "        if 'reasoning' in sample:\n",
        "            reasoning_vals.add(sample['reasoning'])\n",
        "    print(f\"  {split}: categories={categories}, reasoning={reasoning_vals}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Sample and Prepare Data\n",
        "\n",
        "Since the datasets are very large (millions of samples), we'll sample a representative subset for visualization. We'll use stratified sampling to maintain the distribution across splits and versions.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Sampling data from Nemotron v1...\n",
            "  chat: sampling 7,466 / 746,622 (1.0%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "290d9b08fafc4bf1bbe0446b14baebec",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  Processing chat:   0%|          | 0/7466 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  code: sampling 18,963 / 1,896,395 (1.0%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bcd3406718504a5bb17f7730233a66d0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  Processing code:   0%|          | 0/18963 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  math: sampling 20,444 / 2,044,407 (1.0%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7c61919f0c564bd390a5140779897101",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  Processing math:   0%|          | 0/20444 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  stem: sampling 206,621 / 20,662,167 (1.0%)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9d66901f53394dc8bec5c3315429d8a2",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  Processing stem:   0%|          | 0/206621 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 57\u001b[39m\n\u001b[32m     54\u001b[39m indices = np.random.choice(total_samples, size=n_samples, replace=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m tqdm(indices, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Processing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msplit_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, leave=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     sample = \u001b[43msplit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mint\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43midx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# Extract text from messages\u001b[39;00m\n\u001b[32m     60\u001b[39m     messages = sample.get(\u001b[33m'\u001b[39m\u001b[33mmessages\u001b[39m\u001b[33m'\u001b[39m, [])\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2876\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   2874\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._format_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33marrow\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpandas\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mpolars\u001b[39m\u001b[33m\"\u001b[39m):\n\u001b[32m   2875\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m Column(\u001b[38;5;28mself\u001b[39m, key)\n\u001b[32m-> \u001b[39m\u001b[32m2876\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2857\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2855\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2856\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2857\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2858\u001b[39m formatted_output = format_table(\n\u001b[32m   2859\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2860\u001b[39m )\n\u001b[32m   2861\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/formatting/formatting.py:615\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    613\u001b[39m \u001b[38;5;66;03m# Query the main table\u001b[39;00m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m615\u001b[39m     pa_subtable = \u001b[43m_query_table\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    616\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    617\u001b[39m     pa_subtable = _query_table_with_indices_mapping(table, key, indices=indices)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/formatting/formatting.py:85\u001b[39m, in \u001b[36m_query_table\u001b[39m\u001b[34m(table, key)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[33;03mQuery a pyarrow Table to extract the subtable that correspond to the given key.\u001b[39;00m\n\u001b[32m     83\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     84\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m85\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfast_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m     87\u001b[39m     key = \u001b[38;5;28mrange\u001b[39m(*key.indices(table.num_rows))\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/nv/lib/python3.12/site-packages/datasets/table.py:148\u001b[39m, in \u001b[36mIndexedTableMixin.fast_slice\u001b[39m\u001b[34m(self, offset, length)\u001b[39m\n\u001b[32m    146\u001b[39m     j = _interpolation_search(\u001b[38;5;28mself\u001b[39m._offsets, offset + length - \u001b[32m1\u001b[39m)\n\u001b[32m    147\u001b[39m     batches = \u001b[38;5;28mself\u001b[39m._batches[i : j + \u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m148\u001b[39m     batches[-\u001b[32m1\u001b[39m] = \u001b[43mbatches\u001b[49m\u001b[43m[\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     batches[\u001b[32m0\u001b[39m] = batches[\u001b[32m0\u001b[39m].slice(offset - \u001b[38;5;28mself\u001b[39m._offsets[i])\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_batches(batches, schema=\u001b[38;5;28mself\u001b[39m._schema)\n",
            "\u001b[31mKeyboardInterrupt\u001b[39m: "
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "# os.remove(\"embeddings_cache.pkl\", exist_ok=True)\n",
        "SAMPLING_FRACTION = 0.01  # Fraction of data to sample (0.0 to 1.0, where 1.0 = 100%)\n",
        "MAX_TEXT_LENGTH = 2000     # Max characters for embedding (API will auto-truncate to 512 token limit)\n",
        "\n",
        "# Custom color scheme for categories\n",
        "CATEGORY_COLORS = {\n",
        "    'chat': 'red',\n",
        "    'code': 'darkorange', \n",
        "    'math': 'gold',\n",
        "    'stem': 'turquoise',\n",
        "    'tool_calling': 'darkgreen',\n",
        "    'multilingual_ja': 'purple',\n",
        "    'multilingual_de': 'pink',\n",
        "    'multilingual_it': 'brown',\n",
        "    'multilingual_es': 'olive',\n",
        "    'multilingual_fr': 'cyan'\n",
        "}\n",
        "\n",
        "def extract_text_from_messages(messages):\n",
        "    \"\"\"Extract text from messages field.\"\"\"\n",
        "    if not isinstance(messages, list):\n",
        "        return \"\"\n",
        "    \n",
        "    texts = []\n",
        "    for msg in messages:\n",
        "        if isinstance(msg, dict):\n",
        "            # Extract content from message\n",
        "            content = msg.get('content', '')\n",
        "            role = msg.get('role', '')\n",
        "            if content:\n",
        "                texts.append(f\"{role}: {content}\")\n",
        "    \n",
        "    return ' '.join(texts)[:MAX_TEXT_LENGTH]\n",
        "\n",
        "def get_category_and_reasoning(sample):\n",
        "    \"\"\"Extract category and reasoning fields.\"\"\"\n",
        "    category = sample.get('category', 'unknown')\n",
        "    reasoning = sample.get('reasoning', 'unknown')\n",
        "    return category, reasoning\n",
        "\n",
        "# Sample data from both datasets\n",
        "sampled_data = []\n",
        "\n",
        "print(\"üìä Sampling data from Nemotron v1...\")\n",
        "for split_name in dataset_v1.keys():\n",
        "    split_data = dataset_v1[split_name]\n",
        "    total_samples = len(split_data)\n",
        "    n_samples = max(1, int(total_samples * SAMPLING_FRACTION))  # At least 1 sample\n",
        "    \n",
        "    print(f\"  {split_name}: sampling {n_samples:,} / {total_samples:,} ({SAMPLING_FRACTION*100:.1f}%)\")\n",
        "    \n",
        "    # Random sampling\n",
        "    indices = np.random.choice(total_samples, size=n_samples, replace=False)\n",
        "    \n",
        "    for idx in tqdm(indices, desc=f\"  Processing {split_name}\", leave=False):\n",
        "        sample = split_data[int(idx)]\n",
        "        \n",
        "        # Extract text from messages\n",
        "        messages = sample.get('messages', [])\n",
        "        text = extract_text_from_messages(messages)\n",
        "        \n",
        "        # Get category and reasoning\n",
        "        category, reasoning = get_category_and_reasoning(sample)\n",
        "        \n",
        "        sampled_data.append({\n",
        "            'text': text,\n",
        "            'version': 'v1',\n",
        "            'split': split_name,\n",
        "            'category': category,\n",
        "            'reasoning': reasoning,\n",
        "            'idx': int(idx)\n",
        "        })\n",
        "\n",
        "print(f\"  ‚úÖ Sampled {len(sampled_data)} samples from v1\\n\")\n",
        "\n",
        "v1_count = len(sampled_data)\n",
        "\n",
        "print(\"üìä Sampling data from Nemotron v2...\")\n",
        "for split_name in dataset_v2.keys():\n",
        "    split_data = dataset_v2[split_name]\n",
        "    total_samples = len(split_data)\n",
        "    n_samples = max(1, int(total_samples * SAMPLING_FRACTION))  # At least 1 sample\n",
        "    \n",
        "    print(f\"  {split_name}: sampling {n_samples:,} / {total_samples:,} ({SAMPLING_FRACTION*100:.1f}%)\")\n",
        "    \n",
        "    # Random sampling\n",
        "    indices = np.random.choice(total_samples, size=n_samples, replace=False)\n",
        "    \n",
        "    for idx in tqdm(indices, desc=f\"  Processing {split_name}\", leave=False):\n",
        "        sample = split_data[int(idx)]\n",
        "        \n",
        "        # Extract text from messages\n",
        "        messages = sample.get('messages', [])\n",
        "        text = extract_text_from_messages(messages)\n",
        "        \n",
        "        # Get category and reasoning\n",
        "        category, reasoning = get_category_and_reasoning(sample)\n",
        "        \n",
        "        sampled_data.append({\n",
        "            'text': text,\n",
        "            'version': 'v2',\n",
        "            'split': split_name,\n",
        "            'category': category,\n",
        "            'reasoning': reasoning,\n",
        "            'idx': int(idx)\n",
        "        })\n",
        "\n",
        "print(f\"  ‚úÖ Sampled {len(sampled_data) - v1_count} samples from v2\\n\")\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(sampled_data)\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(f\"üìä Total samples prepared: {len(df):,}\")\n",
        "print(f\"   Sampling fraction: {SAMPLING_FRACTION*100:.1f}%\")\n",
        "print(f\"   - v1: {len(df[df['version'] == 'v1']):,}\")\n",
        "print(f\"   - v2: {len(df[df['version'] == 'v2']):,}\")\n",
        "print(f\"\\nCategory distribution:\")\n",
        "print(df['category'].value_counts())\n",
        "print(f\"\\nReasoning distribution:\")\n",
        "print(df['reasoning'].value_counts())\n",
        "print(f\"\\nSplit distribution:\")\n",
        "print(df['split'].value_counts())\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üöÄ Generate Embeddings with NVIDIA NIM\n",
        "\n",
        "Process the text samples in parallel to generate embeddings using NVIDIA's NV-EmbedQA-E5-V5 model.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Embedding cache file\n",
        "EMBEDDINGS_CACHE_FILE = \"embeddings_cache.pkl\"\n",
        "\n",
        "def get_embedding_batch(texts, model=EMBEDDING_MODEL, max_retries=3):\n",
        "    \"\"\"Get embeddings for a batch of texts with retry logic.\"\"\"\n",
        "    for attempt in range(max_retries):\n",
        "        try:\n",
        "            response = client.embeddings.create(\n",
        "                input=texts,\n",
        "                model=model,\n",
        "                # max_tokens=4096,\n",
        "                encoding_format=\"float\",\n",
        "                extra_body={\"input_type\": \"passage\", \"truncate\": \"NONE\"}\n",
        "            )\n",
        "            return [data.embedding for data in response.data]\n",
        "        except Exception as e:\n",
        "            if attempt < max_retries - 1:\n",
        "                wait_time = (attempt + 1) * 2\n",
        "                print(f\"   ‚ö†Ô∏è  Error: {e}. Retrying in {wait_time}s...\")\n",
        "                time.sleep(wait_time)\n",
        "            else:\n",
        "                print(f\"   ‚ùå Failed after {max_retries} attempts: {e}\")\n",
        "                return None\n",
        "    return None\n",
        "\n",
        "def process_batch(batch_texts, batch_indices):\n",
        "    \"\"\"Process a batch of texts and return embeddings with indices.\"\"\"\n",
        "    embeddings = get_embedding_batch(batch_texts)\n",
        "    if embeddings:\n",
        "        return list(zip(batch_indices, embeddings))\n",
        "    return []\n",
        "\n",
        "# Check if embeddings are already cached\n",
        "if os.path.exists(EMBEDDINGS_CACHE_FILE):\n",
        "    print(f\"üì¶ Loading cached embeddings from {EMBEDDINGS_CACHE_FILE}...\")\n",
        "    with open(EMBEDDINGS_CACHE_FILE, 'rb') as f:\n",
        "        embeddings_array = pickle.load(f)\n",
        "    print(f\"‚úÖ Loaded {len(embeddings_array)} cached embeddings\\n\")\n",
        "else:\n",
        "    print(\"üöÄ Generating embeddings using NVIDIA NIM...\\n\")\n",
        "    \n",
        "    # Batch processing configuration\n",
        "    BATCH_SIZE = 32  # Process multiple texts per API call\n",
        "    MAX_WORKERS = 8  # Number of parallel workers\n",
        "    \n",
        "    # Prepare batches\n",
        "    texts = df['text'].tolist()\n",
        "    n_samples = len(texts)\n",
        "    batches = []\n",
        "    \n",
        "    for i in range(0, n_samples, BATCH_SIZE):\n",
        "        batch_texts = texts[i:i+BATCH_SIZE]\n",
        "        batch_indices = list(range(i, min(i+BATCH_SIZE, n_samples)))\n",
        "        batches.append((batch_texts, batch_indices))\n",
        "    \n",
        "    print(f\"üìä Processing {n_samples} texts in {len(batches)} batches (batch size: {BATCH_SIZE})\")\n",
        "    print(f\"   Using {MAX_WORKERS} parallel workers\\n\")\n",
        "    \n",
        "    # Process batches in parallel\n",
        "    embeddings_dict = {}\n",
        "    \n",
        "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
        "        # Submit all batches\n",
        "        future_to_batch = {\n",
        "            executor.submit(process_batch, batch_texts, batch_indices): i \n",
        "            for i, (batch_texts, batch_indices) in enumerate(batches)\n",
        "        }\n",
        "        \n",
        "        # Process results with progress bar\n",
        "        with tqdm(total=len(batches), desc=\"Generating embeddings\") as pbar:\n",
        "            for future in as_completed(future_to_batch):\n",
        "                batch_idx = future_to_batch[future]\n",
        "                try:\n",
        "                    results = future.result()\n",
        "                    for idx, embedding in results:\n",
        "                        embeddings_dict[idx] = embedding\n",
        "                except Exception as e:\n",
        "                    print(f\"\\n‚ùå Batch {batch_idx} failed: {e}\")\n",
        "                pbar.update(1)\n",
        "    \n",
        "    # Convert to numpy array in correct order\n",
        "    embeddings_array = np.array([embeddings_dict[i] for i in range(n_samples) if i in embeddings_dict])\n",
        "    \n",
        "    print(f\"\\n‚úÖ Generated embeddings for {len(embeddings_array)}/{n_samples} samples\")\n",
        "    print(f\"   Embedding shape: {embeddings_array.shape}\")\n",
        "    \n",
        "    # Save embeddings to cache\n",
        "    print(f\"\\nüíæ Saving embeddings to {EMBEDDINGS_CACHE_FILE}...\")\n",
        "    with open(EMBEDDINGS_CACHE_FILE, 'wb') as f:\n",
        "        pickle.dump(embeddings_array, f)\n",
        "    print(\"‚úÖ Embeddings cached successfully!\")\n",
        "\n",
        "# Add embeddings to dataframe (only for successfully embedded samples)\n",
        "if len(embeddings_array) == len(df):\n",
        "    df['embedding'] = list(embeddings_array)\n",
        "    print(f\"\\n‚úÖ All {len(df)} samples have embeddings!\")\n",
        "else:\n",
        "    # Handle case where some embeddings failed\n",
        "    print(f\"\\n‚ö†Ô∏è  Only {len(embeddings_array)}/{len(df)} samples have embeddings\")\n",
        "    df = df.iloc[:len(embeddings_array)].copy()\n",
        "    df['embedding'] = list(embeddings_array)\n",
        "    print(f\"   Trimmed dataframe to {len(df)} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üó∫Ô∏è UMAP Dimensionality Reduction\n",
        "\n",
        "Apply UMAP to reduce the high-dimensional embeddings to 2D for visualization.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üó∫Ô∏è Applying UMAP dimensionality reduction...\\n\")\n",
        "\n",
        "# Prepare embedding matrix\n",
        "embedding_matrix = np.array(df['embedding'].tolist())\n",
        "print(f\"Embedding matrix shape: {embedding_matrix.shape}\")\n",
        "\n",
        "# Apply UMAP\n",
        "reducer = umap.UMAP(\n",
        "    n_components=2,\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric='cosine',\n",
        "    random_state=42,\n",
        "    verbose=True\n",
        ")\n",
        "\n",
        "print(\"\\nFitting UMAP...\")\n",
        "umap_embeddings = reducer.fit_transform(embedding_matrix)\n",
        "\n",
        "# Add UMAP coordinates to dataframe\n",
        "df['umap_x'] = umap_embeddings[:, 0]\n",
        "df['umap_y'] = umap_embeddings[:, 1]\n",
        "\n",
        "print(f\"\\n‚úÖ UMAP reduction complete!\")\n",
        "print(f\"   2D coordinates shape: {umap_embeddings.shape}\")\n",
        "print(f\"   X range: [{umap_embeddings[:, 0].min():.2f}, {umap_embeddings[:, 0].max():.2f}]\")\n",
        "print(f\"   Y range: [{umap_embeddings[:, 1].min():.2f}, {umap_embeddings[:, 1].max():.2f}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Map categories to numerical scores (0-4) for color mapping\n",
        "CATEGORY_TO_SCORE = {\n",
        "    'chat': 0,\n",
        "    'code': 1,\n",
        "    'math': 2,\n",
        "    'stem': 3,\n",
        "    'tool_calling': 4,\n",
        "    # Multilingual categories mapped to existing scores\n",
        "    'multilingual_ja': 2,\n",
        "    'multilingual_de': 3,\n",
        "    'multilingual_it': 1,\n",
        "    'multilingual_es': 0,\n",
        "    'multilingual_fr': 4\n",
        "}\n",
        "\n",
        "# Color mapping for scores 0-4 (matching matplotlib style)\n",
        "SCORE_COLORS = [\"red\", \"darkorange\", \"gold\", \"turquoise\", \"darkgreen\"]\n",
        "\n",
        "# Add Score column based on category\n",
        "df['Score'] = df['category'].map(CATEGORY_TO_SCORE)\n",
        "\n",
        "print(\"Score mapping:\")\n",
        "for cat, score in sorted(CATEGORY_TO_SCORE.items(), key=lambda x: x[1]):\n",
        "    color = SCORE_COLORS[score]\n",
        "    count = len(df[df['category'] == cat])\n",
        "    print(f\"  Score {score} ({color:12s}): {cat:20s} - {count:,} samples\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìä Visualizations\n",
        "\n",
        "Create multiple visualizations of the UMAP 2D projection, colored by different attributes from the dataset headers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 1: Score-based coloring with centroids (matching matplotlib style)\n",
        "print(\"üìä Creating interactive visualizations with Plotly...\\n\")\n",
        "\n",
        "# Extract coordinates\n",
        "x = df['umap_x'].values\n",
        "y = df['umap_y'].values\n",
        "color_indices = df['Score'].values\n",
        "\n",
        "# Create discrete color map for scores\n",
        "score_color_map = {i: SCORE_COLORS[i] for i in range(5)}\n",
        "\n",
        "# Create figure\n",
        "fig = go.Figure()\n",
        "\n",
        "# Add scatter plot for each score\n",
        "for score in range(5):\n",
        "    mask = df['Score'] == score\n",
        "    if mask.sum() > 0:\n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=df[mask]['umap_x'],\n",
        "            y=df[mask]['umap_y'],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                color=SCORE_COLORS[score],\n",
        "                size=8,\n",
        "                opacity=0.3,\n",
        "                line=dict(width=0.5, color='white')\n",
        "            ),\n",
        "            name=f'Score {score}',\n",
        "            customdata=df[mask][['category', 'version', 'split', 'reasoning']].values,\n",
        "            hovertemplate='<b>Score %{text}</b><br>' +\n",
        "                         'Category: %{customdata[0]}<br>' +\n",
        "                         'Version: %{customdata[1]}<br>' +\n",
        "                         'Split: %{customdata[2]}<br>' +\n",
        "                         'Reasoning: %{customdata[3]}<br>' +\n",
        "                         'X: %{x:.2f}<br>' +\n",
        "                         'Y: %{y:.2f}<extra></extra>',\n",
        "            text=[score] * mask.sum()\n",
        "        ))\n",
        "\n",
        "# Calculate and add centroids for each score\n",
        "for score in range(5):\n",
        "    mask = df['Score'] == score\n",
        "    if mask.sum() > 0:\n",
        "        avg_x = df[mask]['umap_x'].mean()\n",
        "        avg_y = df[mask]['umap_y'].mean()\n",
        "        \n",
        "        fig.add_trace(go.Scatter(\n",
        "            x=[avg_x],\n",
        "            y=[avg_y],\n",
        "            mode='markers',\n",
        "            marker=dict(\n",
        "                symbol='x',\n",
        "                size=15,\n",
        "                color=SCORE_COLORS[score],\n",
        "                line=dict(width=3)\n",
        "            ),\n",
        "            name=f'Score {score} centroid',\n",
        "            showlegend=False,\n",
        "            hovertemplate=f'<b>Score {score} Centroid</b><br>' +\n",
        "                         f'X: {avg_x:.2f}<br>' +\n",
        "                         f'Y: {avg_y:.2f}<extra></extra>'\n",
        "        ))\n",
        "\n",
        "fig.update_layout(\n",
        "    title='UMAP Visualization - Colored by Score',\n",
        "    title_font_size=20,\n",
        "    title_x=0.5,\n",
        "    xaxis_title='UMAP Dimension 1',\n",
        "    yaxis_title='UMAP Dimension 2',\n",
        "    width=1200,\n",
        "    height=800,\n",
        "    template='plotly_white',\n",
        "    legend=dict(\n",
        "        title='Score',\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=1.01,\n",
        "        bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=1\n",
        "    ),\n",
        "    hovermode='closest'\n",
        ")\n",
        "\n",
        "fig.write_html('umap_by_score.html')\n",
        "print(\"‚úÖ Saved: umap_by_score.html\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 2: Interactive scatter plot colored by Version (v1 vs v2)\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x='umap_x',\n",
        "    y='umap_y',\n",
        "    color='version',\n",
        "    hover_data=['version', 'split', 'reasoning', 'category'],\n",
        "    title='UMAP Visualization - Colored by Dataset Version (v1 vs v2)',\n",
        "    labels={'umap_x': 'UMAP Dimension 1', 'umap_y': 'UMAP Dimension 2'},\n",
        "    width=800,\n",
        "    height=600,\n",
        "    template='plotly_white',\n",
        "    color_discrete_map={'v1': '#FF6B6B', 'v2': '#4ECDC4'}\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')),\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_font_size=20,\n",
        "    title_x=0.5,\n",
        "    legend=dict(\n",
        "        title='Version',\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=1.01,\n",
        "        bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=1\n",
        "    ),\n",
        "    hovermode='closest'\n",
        ")\n",
        "\n",
        "fig.write_html('umap_by_version.html')\n",
        "print(\"‚úÖ Saved: umap_by_version.html\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 3: Interactive scatter plot colored by Reasoning\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x='umap_x',\n",
        "    y='umap_y',\n",
        "    color='reasoning',\n",
        "    hover_data=['version', 'split', 'reasoning', 'category'],\n",
        "    title='UMAP Visualization - Colored by Reasoning',\n",
        "    labels={'umap_x': 'UMAP Dimension 1', 'umap_y': 'UMAP Dimension 2'},\n",
        "    width=800,\n",
        "    height=600,\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(size=8, opacity=0.7, line=dict(width=0.5, color='white')),\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_font_size=20,\n",
        "    title_x=0.5,\n",
        "    legend=dict(\n",
        "        title='Reasoning',\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=1.01,\n",
        "        bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=1\n",
        "    ),\n",
        "    hovermode='closest'\n",
        ")\n",
        "\n",
        "fig.write_html('umap_by_reasoning.html')\n",
        "print(\"‚úÖ Saved: umap_by_reasoning.html\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 4: Side-by-side comparison with facets (v1 vs v2)\n",
        "fig = px.scatter(\n",
        "    df,\n",
        "    x='umap_x',\n",
        "    y='umap_y',\n",
        "    color='category',\n",
        "    facet_col='version',\n",
        "    hover_data=['version', 'split', 'reasoning', 'category'],\n",
        "    title='UMAP Visualization - v1 vs v2 Comparison (Faceted by Version)',\n",
        "    labels={'umap_x': 'UMAP Dimension 1', 'umap_y': 'UMAP Dimension 2'},\n",
        "    width=800,\n",
        "    height=600,\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(size=7, opacity=0.7, line=dict(width=0.5, color='white')),\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_font_size=20,\n",
        "    title_x=0.5,\n",
        "    legend=dict(\n",
        "        title='Category',\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=1.01,\n",
        "        bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=1\n",
        "    ),\n",
        "    hovermode='closest'\n",
        ")\n",
        "\n",
        "fig.for_each_annotation(lambda a: a.update(text=a.text.replace(\"version=\", \"Nemotron \")))\n",
        "\n",
        "fig.write_html('umap_v1_vs_v2_comparison.html')\n",
        "print(\"‚úÖ Saved: umap_v1_vs_v2_comparison.html\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualization 5: 3D scatter plot with all attributes\n",
        "# Create a combined categorical label for better visualization\n",
        "df['combined_label'] = df['version'] + ' - ' + df['category']\n",
        "\n",
        "fig = px.scatter_3d(\n",
        "    df,\n",
        "    x='umap_x',\n",
        "    y='umap_y',\n",
        "    z=df.groupby('category').ngroup(),  # Use category as third dimension\n",
        "    color='category',\n",
        "    symbol='version',\n",
        "    hover_data=['version', 'split', 'reasoning', 'category'],\n",
        "    title='UMAP Visualization - 3D View with Category Grouping',\n",
        "    labels={\n",
        "        'umap_x': 'UMAP Dimension 1', \n",
        "        'umap_y': 'UMAP Dimension 2',\n",
        "        'z': 'Category Group'\n",
        "    },\n",
        "    width=800,\n",
        "    height=600,\n",
        "    template='plotly_white'\n",
        ")\n",
        "\n",
        "fig.update_traces(\n",
        "    marker=dict(size=5, opacity=0.7, line=dict(width=0.3, color='white')),\n",
        ")\n",
        "\n",
        "fig.update_layout(\n",
        "    title_font_size=20,\n",
        "    title_x=0.5,\n",
        "    scene=dict(\n",
        "        xaxis_title='UMAP Dimension 1',\n",
        "        yaxis_title='UMAP Dimension 2',\n",
        "        zaxis_title='Category Group',\n",
        "        camera=dict(\n",
        "            eye=dict(x=1.5, y=1.5, z=1.3)\n",
        "        )\n",
        "    ),\n",
        "    legend=dict(\n",
        "        yanchor=\"top\",\n",
        "        y=0.99,\n",
        "        xanchor=\"left\",\n",
        "        x=0.01,\n",
        "        bgcolor=\"rgba(255, 255, 255, 0.9)\",\n",
        "        bordercolor=\"gray\",\n",
        "        borderwidth=1\n",
        "    )\n",
        ")\n",
        "\n",
        "fig.write_html('umap_3d_visualization.html')\n",
        "print(\"‚úÖ Saved: umap_3d_visualization.html\")\n",
        "fig.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## üìà Summary Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display summary statistics\n",
        "print(\"=\" * 80)\n",
        "print(\"üìä VISUALIZATION SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(f\"\\n‚úÖ Total samples visualized: {len(df):,}\")\n",
        "print(f\"\\nüìÅ Saved files:\")\n",
        "print(\"   ‚Ä¢ umap_by_category.html - Interactive plot colored by category\")\n",
        "print(\"   ‚Ä¢ umap_by_version.html - Interactive plot showing v1 vs v2\")\n",
        "print(\"   ‚Ä¢ umap_by_reasoning.html - Interactive plot colored by reasoning\")\n",
        "print(\"   ‚Ä¢ umap_v1_vs_v2_comparison.html - Side-by-side comparison\")\n",
        "print(\"   ‚Ä¢ umap_3d_visualization.html - 3D interactive visualization\")\n",
        "\n",
        "print(f\"\\nüìä Dataset Distribution:\")\n",
        "print(f\"\\nBy Version:\")\n",
        "print(df['version'].value_counts())\n",
        "print(f\"\\nBy Category:\")\n",
        "print(df['category'].value_counts())\n",
        "print(f\"\\nBy Reasoning:\")\n",
        "print(df['reasoning'].value_counts())\n",
        "print(f\"\\nBy Split:\")\n",
        "print(df['split'].value_counts())\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ Visualization complete! Open the HTML files in a browser for interactive exploration.\")\n",
        "print(\"=\" * 80)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "nv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
