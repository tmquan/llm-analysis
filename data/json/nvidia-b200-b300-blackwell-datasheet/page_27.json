[
  {
    "page": 27,
    "block_id": 0,
    "type": "Page-header",
    "text": "Conclusion",
    "bbox_normalized": {
      "xmin": 0.7033177083333334,
      "ymin": 0.1797,
      "xmax": 0.7525645833333333,
      "ymax": 0.1844
    },
    "bbox_pixels": {
      "x0": 1080,
      "y0": 368,
      "x1": 1155,
      "y1": 377
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 1,
    "type": "Title",
    "text": "# NVIDIA Blackwell Architecture’s Role in the Age of AI Reasoning",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.2391,
      "xmax": 0.7389385416666667,
      "ymax": 0.2719
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 489,
      "x1": 1135,
      "y1": 556
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 2,
    "type": "Text",
    "text": "AI has evolved to need three distinct scaling laws that describe how applying compute resources in different ways impacts model performance.",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.2797,
      "xmax": 0.7525645833333333,
      "ymax": 0.3
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 572,
      "x1": 1155,
      "y1": 614
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 3,
    "type": "Text",
    "text": "**Pre-training Scaling:** the original law of AI development. It demonstrated that by increasing training dataset size, model parameter count and computational resources, developers could expect predictable improvements in model intelligence and accuracy. Pre-training scaling led to models that achieved groundbreaking capabilities. It spurred major innovations in model architecture, including the rise of billion- and trillion-parameter transformer models, growing 50 million-X in five years in compute requirements.",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.5344,
      "xmax": 0.7493458333333334,
      "ymax": 0.6
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 1094,
      "x1": 1150,
      "y1": 1228
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 4,
    "type": "Text",
    "text": "**Post-training scaling:** While pre-training scaling teaches a model the knowledge of the Internet, post-training teaches a model how to think and further improves a model’s specificity and relevance for an organization’s desired use case. While pretraining is like sending an AI model to school to learn foundational skills, post-training enhances the model with skills applicable to its intended job. To support post-training, developers can use synthetic data to augment or complement their fine-tuning dataset. The total compute required for post-training is 30X more than the compute required for pre- training.",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.6133,
      "xmax": 0.7347541666666667,
      "ymax": 0.7016
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 1256,
      "x1": 1128,
      "y1": 1436
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 5,
    "type": "Text",
    "text": "**Test time scaling (also known as long thinking or reasoning):** LLMs generate quick responses to input prompts and can provide the right answers to simple questions, but it may not be as effective for complex queries. An essential capability for agentic AI workloads requires the LLM to reason through the question before coming up with an",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.7148,
      "xmax": 0.7431229166666666,
      "ymax": 0.7578
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 1463,
      "x1": 1141,
      "y1": 1551
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 6,
    "type": "Page-footer",
    "text": "27<br> NVIDIA Blackwell Architecture Technical Brief",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.7727,
      "xmax": 0.7525645833333333,
      "ymax": 0.7906
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 1582,
      "x1": 1155,
      "y1": 1619
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 7,
    "type": "Picture",
    "text": "FROM ONE TO THREE SCALING LAWS\nTEST-TIME SCALING \"LONG THINKING:\nPOST-TRAINING SCALING\n\"INTELLIGENCE\"\nPRE-TRAINING SCALING\nCOMPUTE",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.3117,
      "xmax": 0.622634375,
      "ymax": 0.4992
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 638,
      "x1": 956,
      "y1": 1022
    },
    "is_valid": true
  },
  {
    "page": 27,
    "block_id": 8,
    "type": "Caption",
    "text": "Figure 13. Three AI Scaling Laws",
    "bbox_normalized": {
      "xmin": 0.245396875,
      "ymin": 0.5141,
      "xmax": 0.466525,
      "ymax": 0.5242
    },
    "bbox_pixels": {
      "x0": 376,
      "y0": 1052,
      "x1": 716,
      "y1": 1073
    },
    "is_valid": true
  }
]