[
  {
    "page": 29,
    "block_id": 0,
    "type": "Page-header",
    "text": "Appendix A",
    "bbox_normalized": {
      "xmin": 0.701171875,
      "ymin": 0.1789,
      "xmax": 0.7535302083333333,
      "ymax": 0.1867
    },
    "bbox_pixels": {
      "x0": 1077,
      "y0": 366,
      "x1": 1157,
      "y1": 382
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 1,
    "type": "Title",
    "text": "## Appendix A",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.2391,
      "xmax": 0.35225937500000004,
      "ymax": 0.2531
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 489,
      "x1": 541,
      "y1": 518
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 2,
    "type": "Section-header",
    "text": "### Advanced Parallelism Techniques in AI Inference for Trillion- Parameter Models",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.2672,
      "xmax": 0.6812156250000001,
      "ymax": 0.2898
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 547,
      "x1": 1046,
      "y1": 593
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 3,
    "type": "Text",
    "text": "The deployment of trillion-parameter models like the GPT 1.8T MoE (Mixture of Experts) presents unique challenges in AI inference, particularly in managing computational resources effectively while ensuring optimal user experience. This appendix explores the various parallelism techniques that can be employed to address these challenges, focusing on data, tensor, pipeline, and expert parallelism.",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.3039,
      "xmax": 0.75041875,
      "ymax": 0.3516
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 622,
      "x1": 1152,
      "y1": 720
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 4,
    "type": "Section-header",
    "text": "### Parallelism Techniques in AI Inference",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.3672,
      "xmax": 0.5146989583333333,
      "ymax": 0.3781
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 752,
      "x1": 790,
      "y1": 774
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 5,
    "type": "List-item",
    "text": "1. **Data Parallelism (DP) Data parallelism** involves hosting multiple copies of the entire model across different GPUs or clusters, processing independent user requests simultaneously. This approach scales linearly with the number of GPUs, enhancing throughput without impacting user interactivity. However, it requires significant memory as each GPU holds a complete model copy.",
    "bbox_normalized": {
      "xmin": 0.26524583333333335,
      "ymin": 0.3836,
      "xmax": 0.7493458333333334,
      "ymax": 0.4313
    },
    "bbox_pixels": {
      "x0": 407,
      "y0": 785,
      "x1": 1150,
      "y1": 883
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 6,
    "type": "List-item",
    "text": "2. **Tensor Parallelism (TP)** Tensor parallelism splits each layer of the model across multiple GPUs, allowing different parts of a user request to be processed in parallel. This method can improve user interactivity by allocating more resources per request, thus reducing processing time. However, it relies heavily on high-bandwidth inter-GPU communication, which can become a bottleneck at large scales.",
    "bbox_normalized": {
      "xmin": 0.26524583333333335,
      "ymin": 0.4328,
      "xmax": 0.7535302083333333,
      "ymax": 0.4797
    },
    "bbox_pixels": {
      "x0": 407,
      "y0": 886,
      "x1": 1157,
      "y1": 982
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 7,
    "type": "List-item",
    "text": "3. **Pipeline Parallelism (PP)** In pipeline parallelism, different groups of model layers are distributed across GPUs, with each part of a user request processed sequentially across the pipeline. This technique helps manage large models by distributing weights, but may lead to inefficiencies in processing and does not significantly enhance user interactivity.",
    "bbox_normalized": {
      "xmin": 0.26524583333333335,
      "ymin": 0.4828,
      "xmax": 0.7482729166666667,
      "ymax": 0.5195
    },
    "bbox_pixels": {
      "x0": 407,
      "y0": 988,
      "x1": 1149,
      "y1": 1063
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 8,
    "type": "List-item",
    "text": "4. **Expert Parallelism (EP)** Expert parallelism routes requests to specific experts within the model to different GPUs, reducing the interaction with unnecessary parameters. The results, after expert processing, require all-to-all communication over high bandwidth GPU interconnect. It requires complex management of data routing and reassembly, and its effectiveness is limited by the number of available experts.",
    "bbox_normalized": {
      "xmin": 0.26524583333333335,
      "ymin": 0.5219,
      "xmax": 0.7482729166666667,
      "ymax": 0.5687
    },
    "bbox_pixels": {
      "x0": 407,
      "y0": 1068,
      "x1": 1149,
      "y1": 1164
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 9,
    "type": "Section-header",
    "text": "### Combining Parallelism Techniques",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.5852,
      "xmax": 0.487446875,
      "ymax": 0.5953
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 1198,
      "x1": 748,
      "y1": 1219
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 10,
    "type": "Text",
    "text": "Combining different parallelism methods can mitigate the limitations of individual techniques. Using both expert and pipeline parallelism can double user interactivity with minimal loss in throughput. Similarly, integrating tensor, expert, and pipeline parallelism can triple GPU throughput without sacrificing user interactivity. Combining different parallelisms for the right deployment scenario is an exhaustive solution space exploration and requires a large set of compute resources.",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.607,
      "xmax": 0.7535302083333333,
      "ymax": 0.6562
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 1243,
      "x1": 1157,
      "y1": 1343
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 11,
    "type": "Section-header",
    "text": "### Maximizing Throughput and Managing Operational Phases",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.6727,
      "xmax": 0.66029375,
      "ymax": 0.6828
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 1377,
      "x1": 1014,
      "y1": 1398
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 12,
    "type": "Text",
    "text": "Efficient management of the prefill and decode phases i.e., context processing and generation phase is crucial for maximizing throughput. Techniques like inflight batching and chunking can optimize GPU utilization by allowing dynamic management of request processing, preventing bottlenecks during these phases.",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.6937,
      "xmax": 0.7189822916666667,
      "ymax": 0.732
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 1420,
      "x1": 1104,
      "y1": 1499
    },
    "is_valid": true
  },
  {
    "page": 29,
    "block_id": 13,
    "type": "Page-footer",
    "text": "29 NVIDIA Blackwell Architecture Technical Brief",
    "bbox_normalized": {
      "xmin": 0.24646979166666663,
      "ymin": 0.7727,
      "xmax": 0.7525645833333333,
      "ymax": 0.7906
    },
    "bbox_pixels": {
      "x0": 378,
      "y0": 1582,
      "x1": 1155,
      "y1": 1619
    },
    "is_valid": true
  }
]